{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcRQZOE1UX55OGfxrlyDud",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikramulhossain/Aiyourvadik/blob/main/final_novel_drug_candidate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy matplotlib seaborn rdkit-pypi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl2182AILRcV",
        "outputId": "56a68e38-04ff-492f-c97e-9bd27adc8877"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement rdkit-pypi (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for rdkit-pypi\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgmVlws9LAFm",
        "outputId": "b355b5c5-a9a9-42dd-a04c-d78ce06c0b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "MACHINE LEARNING DRIVEN DRUG DISCOVERY FOR DIABETES\n",
            "======================================================================\n",
            "\n",
            "========================================\n",
            "STEP 1: DATA PROCESSING\n",
            "========================================\n",
            "Loading dataset...\n",
            "Dataset shape: (440, 27)\n",
            "Missing values per column:\n",
            "medicineNname                             7\n",
            "ActiveIngredient                          7\n",
            "SMILES                                    7\n",
            "Target Protein / Enzyme                   7\n",
            "Protein Binding Affinity (Kd/IC50/Ki)     7\n",
            "Efficacy %                                7\n",
            "Toxicity                                  7\n",
            "Mechanism of Action                       7\n",
            "Absorption                                7\n",
            "Distribution                              7\n",
            "Metabolism                                7\n",
            "Excretion                                 7\n",
            "Bioavailability                           7\n",
            "Bioavailability/Key_Notes                 7\n",
            "Stability                                 7\n",
            "Dose_Range                                7\n",
            "Selectivity                               7\n",
            "Potency                                   7\n",
            "Agonist/Antagonist Activity               7\n",
            "Side Effects                              7\n",
            "Drug Interactions                         7\n",
            "Solubility                                7\n",
            "Lipophilicity - LogP                      7\n",
            "pKa                                      20\n",
            "Molecular Weight                          7\n",
            "Manufacturability                         7\n",
            "Patentability                             7\n",
            "dtype: int64\n",
            "\n",
            "Extracting molecular features from SMILES...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[13:37:57] SMILES Parse Error: syntax error while parsing: CC(C)CC@HNC(=O)C@HNC(=O)C@HN\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 8:\n",
            "[13:37:57] CC(C)CC@HNC(=O)C@HNC(=O)C@HN\n",
            "[13:37:57] ~~~~~~~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'CC(C)CC@HNC(=O)C@HNC(=O)C@HN' for input: 'CC(C)CC@HNC(=O)C@HNC(=O)C@HN'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Peptide:\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Peptide:\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Peptide:' for input: 'Peptide:'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n",
            "[13:37:57] SMILES Parse Error: syntax error while parsing: Protein/Peptide\n",
            "[13:37:57] SMILES Parse Error: check for mistakes around position 2:\n",
            "[13:37:57] Protein/Peptide\n",
            "[13:37:57] ~^\n",
            "[13:37:57] SMILES Parse Error: Failed parsing SMILES 'Protein/Peptide' for input: 'Protein/Peptide'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 40 molecular features\n",
            "Created target variables: ['is_diabetes_drug', 'high_efficacy', 'Bioavailability_numeric']\n",
            "\n",
            "Processed dataset shape: (418, 116)\n",
            "Numerical features: 47\n",
            "Categorical features: 40\n",
            "Target variables: ['is_diabetes_drug', 'high_efficacy', 'Bioavailability_numeric']\n",
            "\n",
            "========================================\n",
            "STEP 2: MACHINE LEARNING MODELING\n",
            "========================================\n",
            "\n",
            "Training models for: is_diabetes_drug\n",
            "Problem type: classification\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 33, number of negative: 301\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1339\n",
            "[LightGBM] [Info] Number of data points in the train set: 334, number of used features: 36\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098802 -> initscore=-2.210603\n",
            "[LightGBM] [Info] Start training from score -2.210603\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 33, number of negative: 301\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1339\n",
            "[LightGBM] [Info] Number of data points in the train set: 334, number of used features: 36\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098802 -> initscore=-2.210603\n",
            "[LightGBM] [Info] Start training from score -2.210603\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training Deep Learning model...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys # Added for sys.modules check\n",
        "\n",
        "# Attempt to install rdkit if not already present and running in Colab\n",
        "if 'google.colab' in sys.modules and 'rdkit' not in sys.modules:\n",
        "    print(\"RDKit not found, attempting installation with 'pip install rdkit'...\")\n",
        "    !pip install rdkit\n",
        "    print(\"RDKit installation initiated. A runtime restart (Runtime -> Restart runtime) is strongly recommended for RDKit to be fully functional.\")\n",
        "    print(\"Proceeding without restart for this execution, but if errors persist, please restart.\")\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Lipinski, PandasTools, Draw\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import DataStructs\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                            roc_auc_score, confusion_matrix, classification_report,\n",
        "                            mean_squared_error, r2_score, mean_absolute_error)\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Deep Learning Libraries\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers, models\n",
        "    DEEP_LEARNING_AVAILABLE = True\n",
        "except:\n",
        "    DEEP_LEARNING_AVAILABLE = False\n",
        "    print(\"TensorFlow not available. Deep learning models will be skipped.\")\n",
        "\n",
        "# ============================================\n",
        "# 1. DATA LOADING AND PREPROCESSING WITH ML\n",
        "# ============================================\n",
        "\n",
        "class DrugDataProcessor:\n",
        "    \"\"\"Process drug dataset and prepare for ML models\"\"\"\n",
        "\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.df = None\n",
        "        self.numerical_features = []\n",
        "        self.categorical_features = []\n",
        "        self.target_columns = []\n",
        "\n",
        "    def load_and_clean(self):\n",
        "        \"\"\"Load and clean the dataset\"\"\"\n",
        "        print(\"Loading dataset...\")\n",
        "        self.df = pd.read_csv(self.filepath)\n",
        "\n",
        "        # Clean column names\n",
        "        self.df.columns = self.df.columns.str.strip().str.replace('ï»¿', '', regex=False)\n",
        "\n",
        "        # Basic info\n",
        "        print(f\"Dataset shape: {self.df.shape}\")\n",
        "        print(f\"Missing values per column:\")\n",
        "        print(self.df.isnull().sum())\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def extract_numerical_features(self):\n",
        "        \"\"\"Extract numerical features from the dataset\"\"\"\n",
        "        numerical_data = {}\n",
        "\n",
        "        # Extract numerical values from string columns\n",
        "        columns_to_extract = [\n",
        "            'Efficacy %', 'Bioavailability', 'Molecular Weight',\n",
        "            'Lipophilicity - LogP', 'pKa'\n",
        "        ]\n",
        "\n",
        "        for col in columns_to_extract:\n",
        "            if col in self.df.columns:\n",
        "                # Extract first numerical value from string\n",
        "                self.df[f'{col}_numeric'] = self.df[col].astype(str).str.extract('([-+]?\\d*\\.\\d+|\\d+)')[0]\n",
        "                self.df[f'{col}_numeric'] = pd.to_numeric(self.df[f'{col}_numeric'], errors='coerce')\n",
        "                numerical_data[col] = self.df[f'{col}_numeric']\n",
        "                self.numerical_features.append(f'{col}_numeric')\n",
        "\n",
        "        # Extract Toxicity score (simplified)\n",
        "        def toxicity_to_score(toxicity):\n",
        "            if pd.isna(toxicity):\n",
        "                return 2  # Medium\n",
        "            toxicity = str(toxicity).lower()\n",
        "            if 'high' in toxicity or 'severe' in toxicity:\n",
        "                return 3\n",
        "            elif 'moderate' in toxicity:\n",
        "                return 2\n",
        "            elif 'low' in toxicity or 'mild' in toxicity:\n",
        "                return 1\n",
        "            else:\n",
        "                return 2\n",
        "\n",
        "        self.df['toxicity_score'] = self.df['Toxicity'].apply(toxicity_to_score)\n",
        "        self.numerical_features.append('toxicity_score')\n",
        "\n",
        "        # Extract Patentability (binary)\n",
        "        self.df['is_patented'] = self.df['Patentability'].apply(\n",
        "            lambda x: 1 if 'brand' in str(x).lower() or 'still' in str(x).lower() else 0\n",
        "        )\n",
        "        self.numerical_features.append('is_patented')\n",
        "\n",
        "        return numerical_data\n",
        "\n",
        "    def extract_molecular_features(self, smiles_column='SMILES'):\n",
        "        \"\"\"Extract molecular descriptors from SMILES using RDKit\"\"\"\n",
        "        print(\"\\nExtracting molecular features from SMILES...\")\n",
        "\n",
        "        molecular_features = []\n",
        "\n",
        "        for idx, row in self.df.iterrows():\n",
        "            if pd.isna(row[smiles_column]):\n",
        "                features = {f'mol_feat_{i}': np.nan for i in range(20)}\n",
        "            else:\n",
        "                mol = Chem.MolFromSmiles(str(row[smiles_column]))\n",
        "                if mol:\n",
        "                    # Basic molecular descriptors\n",
        "                    features = {\n",
        "                        'mol_weight': Descriptors.MolWt(mol),\n",
        "                        'logp': Descriptors.MolLogP(mol),\n",
        "                        'hbd': Descriptors.NumHDonors(mol),\n",
        "                        'hba': Descriptors.NumHAcceptors(mol),\n",
        "                        'tpsa': Descriptors.TPSA(mol),\n",
        "                        'rotatable_bonds': Descriptors.NumRotatableBonds(mol),\n",
        "                        'heavy_atoms': mol.GetNumHeavyAtoms(),\n",
        "                        'ring_count': Descriptors.RingCount(mol),\n",
        "                        'fraction_sp3': Descriptors.FractionCSP3(mol),\n",
        "                        'num_aromatic_rings': Descriptors.NumAromaticRings(mol),\n",
        "                        'num_saturated_rings': Descriptors.NumSaturatedRings(mol),\n",
        "                        'num_heteroatoms': Descriptors.NumHeteroatoms(mol),\n",
        "                        'mol_refractivity': Descriptors.MolMR(mol),\n",
        "                        'balaban_j': Descriptors.BalabanJ(mol) if mol.GetNumAtoms() > 1 else 0,\n",
        "                        'chi0v': Descriptors.Chi0v(mol),\n",
        "                        'chi1v': Descriptors.Chi1v(mol),\n",
        "                        'kappa1': Descriptors.Kappa1(mol),\n",
        "                        'kappa2': Descriptors.Kappa2(mol),\n",
        "                        'kappa3': Descriptors.Kappa3(mol),\n",
        "                        'lipinski_violations': sum([\n",
        "                            1 if Descriptors.MolWt(mol) > 500 else 0,\n",
        "                            1 if Descriptors.MolLogP(mol) > 5 else 0,\n",
        "                            1 if Descriptors.NumHDonors(mol) > 5 else 0,\n",
        "                            1 if Descriptors.NumHAcceptors(mol) > 10 else 0\n",
        "                        ])\n",
        "                    }\n",
        "                else:\n",
        "                    features = {k: np.nan for k in [\n",
        "                        'mol_weight', 'logp', 'hbd', 'hba', 'tpsa',\n",
        "                        'rotatable_bonds', 'heavy_atoms', 'ring_count',\n",
        "                        'fraction_sp3', 'num_aromatic_rings', 'num_saturated_rings',\n",
        "                        'num_heteroatoms', 'mol_refractivity', 'balaban_j',\n",
        "                        'chi0v', 'chi1v', 'kappa1', 'kappa2', 'kappa3',\n",
        "                        'lipinski_violations'\n",
        "                    ]}\n",
        "\n",
        "            molecular_features.append(features)\n",
        "\n",
        "        # Add to dataframe\n",
        "        mol_df = pd.DataFrame(molecular_features)\n",
        "        for col in mol_df.columns:\n",
        "            self.df[col] = mol_df[col]\n",
        "            self.numerical_features.append(col)\n",
        "\n",
        "        print(f\"Added {len(mol_df.columns)} molecular features\")\n",
        "\n",
        "        return mol_df\n",
        "\n",
        "    def encode_categorical_features(self):\n",
        "        \"\"\"Encode categorical features for ML\"\"\"\n",
        "        categorical_cols = [\n",
        "            'Mechanism of Action', 'Agonist/Antagonist Activity',\n",
        "            'Selectivity', 'Manufacturability'\n",
        "        ]\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            if col in self.df.columns:\n",
        "                # Create dummy variables for top categories\n",
        "                top_categories = self.df[col].value_counts().head(10).index\n",
        "                self.df[col] = self.df[col].apply(\n",
        "                    lambda x: x if x in top_categories else 'Other'\n",
        "                )\n",
        "                dummies = pd.get_dummies(self.df[col], prefix=col, drop_first=True)\n",
        "                self.df = pd.concat([self.df, dummies], axis=1)\n",
        "                self.categorical_features.extend(dummies.columns.tolist())\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def create_target_variables(self):\n",
        "        \"\"\"Create target variables for ML models\"\"\"\n",
        "        # Target 1: Diabetes Drug Classification\n",
        "        diabetes_keywords = [\n",
        "            'insulin', 'glucose', 'diabetes', 'glp', 'gip', 'sglt', 'dpp',\n",
        "            'sulfonylurea', 'metformin', 'ppar', 'glucagon', 'incretin'\n",
        "        ]\n",
        "\n",
        "        def is_diabetes_related(text):\n",
        "            if pd.isna(text):\n",
        "                return 0\n",
        "            text = str(text).lower()\n",
        "            return 1 if any(keyword in text for keyword in diabetes_keywords) else 0\n",
        "\n",
        "        self.df['is_diabetes_drug'] = self.df['Target Protein / Enzyme'].apply(is_diabetes_related)\n",
        "        self.target_columns.append('is_diabetes_drug')\n",
        "\n",
        "        # Target 2: High Efficacy (binary classification)\n",
        "        self.df['high_efficacy'] = self.df['Efficacy %_numeric'].apply(\n",
        "            lambda x: 1 if x > 80 else 0 if pd.notna(x) else np.nan\n",
        "        )\n",
        "        self.target_columns.append('high_efficacy')\n",
        "\n",
        "        # Target 3: Bioavailability (regression)\n",
        "        self.target_columns.append('Bioavailability_numeric')\n",
        "\n",
        "        print(f\"Created target variables: {self.target_columns}\")\n",
        "\n",
        "        return self.target_columns\n",
        "\n",
        "    def prepare_ml_data(self):\n",
        "        \"\"\"Prepare clean dataset for ML\"\"\"\n",
        "        # Remove rows with too many missing values\n",
        "        self.df = self.df.dropna(subset=self.target_columns)\n",
        "\n",
        "        # Fill missing numerical values with median\n",
        "        for col in self.numerical_features:\n",
        "            if col in self.df.columns:\n",
        "                self.df[col] = self.df[col].fillna(self.df[col].median())\n",
        "\n",
        "        # Prepare feature matrix X and target y\n",
        "        features = self.numerical_features + self.categorical_features\n",
        "        X = self.df[features]\n",
        "\n",
        "        ml_data = {}\n",
        "        for target in self.target_columns:\n",
        "            if target in self.df.columns:\n",
        "                y = self.df[target]\n",
        "                # Remove rows where target is NaN\n",
        "                mask = y.notna()\n",
        "                ml_data[target] = {\n",
        "                    'X': X[mask],\n",
        "                    'y': y[mask]\n",
        "                }\n",
        "\n",
        "        return ml_data\n",
        "# (Removed problematic code snippet here)\n",
        "\n",
        "# ============================================\n",
        "# 2. MACHINE LEARNING MODELS FOR DRUG DISCOVERY\n",
        "# ============================================\n",
        "\n",
        "class DrugDiscoveryML:\n",
        "    \"\"\"Machine Learning models for drug discovery\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.feature_importance = {}\n",
        "\n",
        "    def train_classification_model(self, X_train, y_train, X_test, y_test, model_type='rf'):\n",
        "        \"\"\"Train classification model\"\"\"\n",
        "        if model_type == 'rf':\n",
        "            model = RandomForestClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=10,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                random_state=42\n",
        "            )\n",
        "        elif model_type == 'svm':\n",
        "            model = SVC(probability=True, random_state=42)\n",
        "        elif model_type == 'logistic':\n",
        "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "        elif model_type == 'xgboost':\n",
        "            model = xgb.XGBClassifier(random_state=42)\n",
        "        elif model_type == 'lightgbm':\n",
        "            model = lgb.LGBMClassifier(random_state=42)\n",
        "        else:\n",
        "            model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_test, y_pred),\n",
        "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "            'f1': f1_score(y_test, y_pred, zero_division=0),\n",
        "            'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
        "        }\n",
        "\n",
        "        if y_pred_proba is not None and len(set(y_test)) > 1:\n",
        "            metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "        # Feature importance\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            self.feature_importance[model_type] = dict(zip(X_train.columns, model.feature_importances_))\n",
        "\n",
        "        return model, metrics\n",
        "\n",
        "    def train_regression_model(self, X_train, y_train, X_test, y_test, model_type='rf'):\n",
        "        \"\"\"Train regression model\"\"\"\n",
        "        if model_type == 'rf':\n",
        "            model = RandomForestRegressor(\n",
        "                n_estimators=100,\n",
        "                max_depth=10,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                random_state=42\n",
        "            )\n",
        "        elif model_type == 'svm':\n",
        "            model = SVR()\n",
        "        elif model_type == 'linear':\n",
        "            model = LinearRegression()\n",
        "        elif model_type == 'xgboost':\n",
        "            model = xgb.XGBRegressor(random_state=42)\n",
        "        elif model_type == 'lightgbm':\n",
        "            model = lgb.LGBMRegressor(random_state=42)\n",
        "        else:\n",
        "            model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'mse': mean_squared_error(y_test, y_pred),\n",
        "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "            'mae': mean_absolute_error(y_test, y_pred),\n",
        "            'r2': r2_score(y_test, y_pred)\n",
        "        }\n",
        "\n",
        "        # Feature importance\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            self.feature_importance[model_type] = dict(zip(X_train.columns, model.feature_importances_))\n",
        "\n",
        "        return model, metrics\n",
        "\n",
        "    def train_deep_learning_model(self, X_train, y_train, X_test, y_test, problem_type='classification'):\n",
        "        \"\"\"Train deep learning model using TensorFlow/Keras\"\"\"\n",
        "        if not DEEP_LEARNING_AVAILABLE:\n",
        "            print(\"Deep learning not available. Skipping.\")\n",
        "            return None, {}\n",
        "\n",
        "        # Identify and remove constant features from training and test sets\n",
        "        # This prevents StandardScaler from producing NaNs/Infs due to zero variance\n",
        "        constant_features = X_train.columns[X_train.nunique() == 1]\n",
        "        X_train_filtered = X_train.drop(columns=constant_features)\n",
        "        X_test_filtered = X_test.drop(columns=constant_features)\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
        "        X_test_scaled = scaler.transform(X_test_filtered)\n",
        "\n",
        "        # Replace any remaining NaNs/Infs (as a safeguard, should be rare after filtering constant features)\n",
        "        X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=1e5, neginf=-1e5)\n",
        "        X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=1e5, neginf=-1e5)\n",
        "\n",
        "        # Build model\n",
        "        model = keras.Sequential([\n",
        "            layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dense(1, activation='sigmoid' if problem_type == 'classification' else 'linear')\n",
        "        ])\n",
        "\n",
        "        # Compile model\n",
        "        if problem_type == 'classification':\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', keras.metrics.AUC()]\n",
        "            )\n",
        "            epochs = 50\n",
        "        else:\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='mse',\n",
        "                metrics=['mae', keras.metrics.RootMeanSquaredError()]\n",
        "            )\n",
        "            epochs = 100\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train_scaled, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        # Ensure y_pred is finite and valid before computing metrics\n",
        "        if problem_type == 'classification':\n",
        "            # For classification, predictions should be between 0 and 1\n",
        "            y_pred = np.nan_to_num(y_pred, nan=0.5, posinf=1.0, neginf=0.0)\n",
        "            y_pred = np.clip(y_pred, 0.0, 1.0) # Clip to ensure valid probability range\n",
        "        else:\n",
        "            y_pred = np.nan_to_num(y_pred, nan=0.0, posinf=1e10, neginf=-1e10) # For regression, use large finite numbers\n",
        "\n",
        "\n",
        "        if problem_type == 'classification':\n",
        "            y_pred_class = (y_pred > 0.5).astype(int)\n",
        "            metrics = {\n",
        "                'accuracy': accuracy_score(y_test, y_pred_class),\n",
        "                'precision': precision_score(y_test, y_pred_class, zero_division=0),\n",
        "                'recall': recall_score(y_test, y_pred_class, zero_division=0),\n",
        "                'f1': f1_score(y_test, y_pred_class, zero_division=0),\n",
        "                'auc': roc_auc_score(y_test, y_pred) if len(set(y_test)) > 1 else 0\n",
        "            }\n",
        "        else:\n",
        "            metrics = {\n",
        "                'mse': mean_squared_error(y_test, y_pred),\n",
        "                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "                'mae': mean_absolute_error(y_test, y_pred),\n",
        "                'r2': r2_score(y_test, y_pred)\n",
        "            }\n",
        "\n",
        "        return model, metrics, history\n",
        "\n",
        "    def ensemble_voting(self, X_train, y_train, X_test, y_test, problem_type='classification'):\n",
        "        \"\"\"Create ensemble of models\"\"\"\n",
        "        if problem_type == 'classification':\n",
        "            from sklearn.ensemble import VotingClassifier\n",
        "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "            xgb_model = xgb.XGBClassifier(random_state=42)\n",
        "            lgb_model = lgb.LGBMClassifier(random_state=42)\n",
        "\n",
        "            ensemble = VotingClassifier(\n",
        "                estimators=[('rf', rf), ('xgb', xgb_model), ('lgb', lgb_model)],\n",
        "                voting='soft'\n",
        "            )\n",
        "        else:\n",
        "            from sklearn.ensemble import VotingRegressor\n",
        "            rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "            xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "            lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "            ensemble = VotingRegressor(\n",
        "                estimators=[('rf', rf), ('xgb', xgb_model), ('lgb', lgb_model)]\n",
        "            )\n",
        "\n",
        "        ensemble.fit(X_train, y_train)\n",
        "        y_pred = ensemble.predict(X_test)\n",
        "\n",
        "        if problem_type == 'classification':\n",
        "            metrics = {\n",
        "                'accuracy': accuracy_score(y_test, y_pred),\n",
        "                'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "                'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "                'f1': f1_score(y_test, y_pred, zero_division=0)\n",
        "            }\n",
        "        else:\n",
        "            metrics = {\n",
        "                'mse': mean_squared_error(y_test, y_pred),\n",
        "                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "                'mae': mean_absolute_error(y_test, y_pred),\n",
        "                'r2': r2_score(y_test, y_pred)\n",
        "            }\n",
        "\n",
        "        return ensemble, metrics\n",
        "\n",
        "    def cross_validate(self, X, y, model_type='rf', cv=5):\n",
        "        \"\"\"Perform cross-validation\"\"\"\n",
        "        if model_type == 'rf':\n",
        "            model = RandomForestClassifier(random_state=42) if len(set(y)) < 10 else RandomForestRegressor(random_state=42)\n",
        "        elif model_type == 'xgboost':\n",
        "            model = xgb.XGBClassifier(random_state=42) if len(set(y)) < 10 else xgb.XGBRegressor(random_state=42)\n",
        "\n",
        "        if len(set(y)) < 10:  # Classification\n",
        "            scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "        else:  # Regression\n",
        "            scores = cross_val_score(model, X, y, cv=cv, scoring='r2')\n",
        "\n",
        "        return scores.mean(), scores.std()\n",
        "\n",
        "# ============================================\n",
        "# 3. VIRTUAL SCREENING AND PREDICTION\n",
        "# ============================================\n",
        "\n",
        "class VirtualScreener:\n",
        "    \"\"\"Virtual screening of compounds for diabetes targets\"\"\"\n",
        "\n",
        "    def __init__(self, trained_models, all_feature_names):\n",
        "        self.models = trained_models\n",
        "        self.predictions = {}\n",
        "        self.all_feature_names = all_feature_names\n",
        "\n",
        "    def predict_diabetes_potential(self, features_df):\n",
        "        \"\"\"Predict diabetes drug potential for new compounds\"\"\"\n",
        "        predictions = {}\n",
        "\n",
        "        # Ensure the input features_df has all columns the model was trained on\n",
        "        missing_cols = set(self.all_feature_names) - set(features_df.columns)\n",
        "        for c in missing_cols:\n",
        "            features_df[c] = 0  # Fill missing (categorical) features with 0\n",
        "        # Ensure the order of columns matches the training order\n",
        "        features_df = features_df[self.all_feature_names]\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "            if hasattr(model, 'predict'):\n",
        "                pred = model.predict(features_df)\n",
        "                predictions[model_name] = pred.item() if isinstance(pred, np.ndarray) and pred.size == 1 else pred\n",
        "\n",
        "                if hasattr(model, 'predict_proba'):\n",
        "                    pred_proba = model.predict_proba(features_df)\n",
        "                    predictions[f'{model_name}_probability'] = pred_proba[0, 1].item() if isinstance(pred_proba[0, 1], np.ndarray) and pred_proba[0, 1].size == 1 else pred_proba[0, 1]\n",
        "\n",
        "        self.predictions = predictions\n",
        "\n",
        "        # Ensemble prediction (average)\n",
        "        if len(predictions) > 0:\n",
        "            # Filter out probability predictions for averaging the base predictions\n",
        "            # And ensure all values are scalars for consistent averaging\n",
        "            scalar_base_preds = []\n",
        "            for name, p in predictions.items():\n",
        "                if '_probability' not in name:\n",
        "                    if isinstance(p, np.ndarray):\n",
        "                        if p.size == 1:\n",
        "                            scalar_base_preds.append(p.item()) # Extract scalar from 1-element array\n",
        "                        else:\n",
        "                            scalar_base_preds.append(p[0]) # Fallback for multi-element array (shouldn't happen for single input)\n",
        "                    elif isinstance(p, (int, float)):\n",
        "                        scalar_base_preds.append(p)\n",
        "\n",
        "            if scalar_base_preds:\n",
        "                avg_proba = np.mean(scalar_base_preds)\n",
        "                predictions['ensemble_average'] = avg_proba\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def screen_smiles(self, smiles_list):\n",
        "        \"\"\"Screen a list of SMILES strings\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for smiles in smiles_list:\n",
        "            # Calculate molecular features\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol:\n",
        "                features = self.calculate_molecular_features(mol)\n",
        "                # Predict using trained models\n",
        "                # Create a DataFrame with molecular features and ensure all feature columns are present\n",
        "                mol_features_df = pd.DataFrame([features])\n",
        "                pred = self.predict_diabetes_potential(mol_features_df)\n",
        "                results.append({\n",
        "                    'smiles': smiles,\n",
        "                    'predictions': pred,\n",
        "                    'drug_likeness': self.calculate_drug_likeness(mol)\n",
        "                })\n",
        "            else:\n",
        "                results.append({\n",
        "                    'smiles': smiles,\n",
        "                    'predictions': {}, # Empty predictions for invalid SMILES\n",
        "                    'drug_likeness': {'drug_score': np.nan, 'is_druglike': False}\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def calculate_molecular_features(self, mol):\n",
        "        \"\"\"Calculate molecular features for prediction\"\"\"\n",
        "        features = {\n",
        "            'mol_weight': Descriptors.MolWt(mol),\n",
        "            'logp': Descriptors.MolLogP(mol),\n",
        "            'hbd': Descriptors.NumHDonors(mol),\n",
        "            'hba': Descriptors.NumHAcceptors(mol),\n",
        "            'tpsa': Descriptors.TPSA(mol),\n",
        "            'rotatable_bonds': Descriptors.NumRotatableBonds(mol),\n",
        "            'heavy_atoms': mol.GetNumHeavyAtoms(),\n",
        "            'ring_count': Descriptors.RingCount(mol),\n",
        "            'fraction_sp3': Descriptors.FractionCSP3(mol),\n",
        "            'num_aromatic_rings': Descriptors.NumAromaticRings(mol),\n",
        "            'num_saturated_rings': Descriptors.NumSaturatedRings(mol), # Added this missing feature\n",
        "            'num_heteroatoms': Descriptors.NumHeteroatoms(mol), # Added this missing feature\n",
        "            'mol_refractivity': Descriptors.MolMR(mol), # Added this missing feature\n",
        "            'balaban_j': Descriptors.BalabanJ(mol) if mol.GetNumAtoms() > 1 else 0, # Added this missing feature\n",
        "            'chi0v': Descriptors.Chi0v(mol), # Added this missing feature\n",
        "            'chi1v': Descriptors.Chi1v(mol), # Added this missing feature\n",
        "            'kappa1': Descriptors.Kappa1(mol), # Added this missing feature\n",
        "            'kappa2': Descriptors.Kappa2(mol), # Added this missing feature\n",
        "            'kappa3': Descriptors.Kappa3(mol), # Added this missing feature\n",
        "            'lipinski_violations': sum([\n",
        "                1 if Descriptors.MolWt(mol) > 500 else 0,\n",
        "                1 if Descriptors.MolLogP(mol) > 5 else 0,\n",
        "                1 if Descriptors.NumHDonors(mol) > 5 else 0,\n",
        "                1 if Descriptors.NumHAcceptors(mol) > 10 else 0\n",
        "            ])\n",
        "        }\n",
        "        return features\n",
        "\n",
        "    def calculate_drug_likeness(self, mol):\n",
        "        \"\"\"Calculate drug-likeness score\"\"\"\n",
        "        # QED (Quantitative Estimate of Drug-likeness)\n",
        "        try:\n",
        "            qed = Descriptors.qed(mol)\n",
        "        except:\n",
        "            qed = 0.5\n",
        "\n",
        "        # Lipinski compliance\n",
        "        lipinski_ok = sum([\n",
        "            1 if Descriptors.MolWt(mol) <= 500 else 0,\n",
        "            1 if Descriptors.MolLogP(mol) <= 5 else 0,\n",
        "            1 if Descriptors.NumHDonors(mol) <= 5 else 0,\n",
        "            1 if Descriptors.NumHAcceptors(mol) <= 10 else 0\n",
        "        ]) / 4\n",
        "\n",
        "        # Combined score\n",
        "        drug_score = (qed * 0.6 + lipinski_ok * 0.4)\n",
        "\n",
        "        return {\n",
        "            'qed': qed,\n",
        "            'lipinski_compliance': lipinski_ok,\n",
        "            'drug_score': drug_score,\n",
        "            'is_druglike': drug_score > 0.5\n",
        "        }\n",
        "\n",
        "# ============================================\n",
        "# 4. VISUALIZATION AND ANALYSIS\n",
        "# ============================================\n",
        "\n",
        "class DrugDiscoveryVisualizer:\n",
        "    \"\"\"Visualization tools for drug discovery\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.figures = {}\n",
        "\n",
        "    def plot_model_performance(self, results_dict, title=\"Model Performance\"):\n",
        "        \"\"\"Plot comparison of model performance\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        # Extract data for plotting\n",
        "        models = list(results_dict.keys())\n",
        "\n",
        "        # Classification metrics\n",
        "        if 'accuracy' in next(iter(results_dict.values())):\n",
        "            metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
        "            metric_data = {metric: [] for metric in metrics}\n",
        "\n",
        "            for model_name, metrics_dict in results_dict.items():\n",
        "                for metric in metrics:\n",
        "                    if metric in metrics_dict:\n",
        "                        metric_data[metric].append(metrics_dict[metric])\n",
        "\n",
        "            for idx, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):\n",
        "                if metric_data[metric]:\n",
        "                    ax = axes[idx // 2, idx % 2]\n",
        "                    bars = ax.bar(models, metric_data[metric])\n",
        "                    ax.set_title(f'{metric.title()} Comparison')\n",
        "                    ax.set_ylabel(metric.title())\n",
        "                    ax.set_ylim(0, 1)\n",
        "                    ax.set_xticklabels(models, rotation=45)\n",
        "\n",
        "                    # Add value labels\n",
        "                    for bar in bars:\n",
        "                        height = bar.get_height()\n",
        "                        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                               f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # Regression metrics\n",
        "        else:\n",
        "            metrics = ['r2', 'rmse', 'mae']\n",
        "            metric_data = {metric: [] for metric in metrics}\n",
        "\n",
        "            for model_name, metrics_dict in results_dict.items():\n",
        "                for metric in metrics:\n",
        "                    if metric in metrics_dict:\n",
        "                        metric_data[metric].append(metrics_dict[metric])\n",
        "\n",
        "            for idx, metric in enumerate(metrics):\n",
        "                ax = axes[idx // 2, idx % 2]\n",
        "                bars = ax.bar(models, metric_data[metric])\n",
        "                ax.set_title(f'{metric.upper()} Comparison')\n",
        "                ax.set_ylabel(metric.upper())\n",
        "                ax.set_xticklabels(models, rotation=45)\n",
        "\n",
        "                # Add value labels\n",
        "                for bar in bars:\n",
        "                    height = bar.get_height()\n",
        "                    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                           f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        plt.suptitle(title, fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('model_performance.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        self.figures['model_performance'] = fig\n",
        "\n",
        "    def plot_feature_importance(self, feature_importance_dict, top_n=20):\n",
        "        \"\"\"Plot feature importance from models\"\"\"\n",
        "        fig, axes = plt.subplots(len(feature_importance_dict), 1,\n",
        "                                figsize=(12, 4*len(feature_importance_dict)))\n",
        "\n",
        "        if len(feature_importance_dict) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for idx, (model_name, importance) in enumerate(feature_importance_dict.items()):\n",
        "            # Sort features by importance\n",
        "            sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "            features, importance_values = zip(*sorted_features)\n",
        "\n",
        "            ax = axes[idx]\n",
        "            bars = ax.barh(range(len(features)), importance_values)\n",
        "            ax.set_yticks(range(len(features)))\n",
        "            ax.set_yticklabels(features)\n",
        "            ax.set_xlabel('Importance')\n",
        "            ax.set_title(f'Feature Importance - {model_name}')\n",
        "            ax.invert_yaxis()  # Most important at top\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        self.figures['feature_importance'] = fig\n",
        "\n",
        "    def plot_cluster_analysis(self, X, y_pred, title=\"Compound Clustering\"):\n",
        "        \"\"\"Visualize clustering results\"\"\"\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "        # PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        X_pca = pca.fit_transform(X)\n",
        "        scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap='viridis', alpha=0.6)\n",
        "        axes[0].set_title('PCA - 2D Projection')\n",
        "        axes[0].set_xlabel('PC1')\n",
        "        axes[0].set_ylabel('PC2')\n",
        "        plt.colorbar(scatter1, ax=axes[0])\n",
        "\n",
        "        # t-SNE\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        X_tsne = tsne.fit_transform(X)\n",
        "        scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred, cmap='plasma', alpha=0.6)\n",
        "        axes[1].set_title('t-SNE - 2D Projection')\n",
        "        axes[1].set_xlabel('t-SNE 1')\n",
        "        axes[1].set_ylabel('t-SNE 2')\n",
        "        plt.colorbar(scatter2, ax=axes[1])\n",
        "\n",
        "        # Cluster distribution\n",
        "        unique, counts = np.unique(y_pred, return_counts=True)\n",
        "        axes[2].bar(unique, counts, color='skyblue', edgecolor='black')\n",
        "        axes[2].set_title('Cluster Distribution')\n",
        "        axes[2].set_xlabel('Cluster')\n",
        "        axes[2].set_ylabel('Number of Compounds')\n",
        "\n",
        "        plt.suptitle(title, fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('cluster_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        self.figures['cluster_analysis'] = fig\n",
        "\n",
        "    def plot_prediction_distribution(self, y_true, y_pred, title=\"Prediction Distribution\"):\n",
        "        \"\"\"Plot distribution of predictions vs actual values\"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "        # Scatter plot for regression\n",
        "        if len(set(y_true)) > 10:  # Likely regression\n",
        "            axes[0].scatter(y_true, y_pred, alpha=0.6)\n",
        "            axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()],\n",
        "                        'r--', lw=2)\n",
        "            axes[0].set_xlabel('Actual Values')\n",
        "            axes[0].set_ylabel('Predicted Values')\n",
        "            axes[0].set_title('Actual vs Predicted')\n",
        "\n",
        "            # Residuals\n",
        "            residuals = y_true - y_pred\n",
        "            axes[1].hist(residuals, bins=30, alpha=0.7, color='salmon', edgecolor='black')\n",
        "            axes[1].axvline(x=0, color='r', linestyle='--')\n",
        "            axes[1].set_xlabel('Residuals')\n",
        "            axes[1].set_ylabel('Frequency')\n",
        "            axes[1].set_title('Residual Distribution')\n",
        "\n",
        "        # Confusion matrix for classification\n",
        "        else:\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "            axes[0].set_xlabel('Predicted')\n",
        "            axes[0].set_ylabel('Actual')\n",
        "            axes[0].set_title('Confusion Matrix')\n",
        "\n",
        "            # Class distribution\n",
        "            axes[1].bar([0, 1], [sum(y_true == 0), sum(y_true == 1)],\n",
        "                       color=['blue', 'red'], alpha=0.7, edgecolor='black')\n",
        "            axes[1].set_xlabel('Class')\n",
        "            axes[1].set_ylabel('Count')\n",
        "            axes[1].set_title('Class Distribution')\n",
        "            axes[1].set_xticks([0, 1])\n",
        "            axes[1].set_xticklabels(['Negative', 'Positive'])\n",
        "\n",
        "        plt.suptitle(title, fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('prediction_distribution.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        self.figures['prediction_distribution'] = fig\n",
        "\n",
        "# ============================================\n",
        "# 5. NOVEL DRUG CANDIDATE GENERATION WITH ML\n",
        "# ============================================\n",
        "\n",
        "class MLDrivenDrugCandidate:\n",
        "    \"\"\"Generate novel drug candidates using ML predictions\"\"\"\n",
        "\n",
        "    def __init__(self, trained_models, feature_names):\n",
        "        self.models = trained_models\n",
        "        self.feature_names = feature_names\n",
        "        self.candidates = []\n",
        "\n",
        "    def generate_candidate_from_smiles(self, smiles, candidate_name=\"ML_Candidate\"):\n",
        "        \"\"\"Generate drug candidate from SMILES using ML predictions\"\"\"\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if not mol:\n",
        "            print(f\"Invalid SMILES: {smiles}\")\n",
        "            return None\n",
        "\n",
        "        # Calculate features\n",
        "        features = self._calculate_all_features(mol)\n",
        "\n",
        "        # Make predictions using all models\n",
        "        predictions = {}\n",
        "        for model_name, model in self.models.items():\n",
        "            if hasattr(model, 'predict'):\n",
        "                try:\n",
        "                    # Prepare features in correct format\n",
        "                    X = pd.DataFrame([features])\n",
        "                    # Ensure the input features_df has all columns the model was trained on\n",
        "                    missing_cols = set(self.feature_names) - set(X.columns)\n",
        "                    for c in missing_cols:\n",
        "                        X[c] = 0  # Fill missing (categorical) features with 0\n",
        "                    # Ensure the order of columns matches the training order\n",
        "                    X = X[self.feature_names]\n",
        "\n",
        "                    pred = model.predict(X)\n",
        "                    predictions[model_name] = pred.item() if isinstance(pred, np.ndarray) and pred.size == 1 else pred\n",
        "\n",
        "                    if hasattr(model, 'predict_proba'):\n",
        "                        pred_proba = model.predict_proba(X)\n",
        "                        predictions[f'{model_name}_probability'] = pred_proba[0, 1].item() if isinstance(pred_proba[0, 1], np.ndarray) and pred_proba[0, 1].size == 1 else pred_proba[0, 1]\n",
        "                except Exception as e:\n",
        "                    print(f\"Prediction failed for {model_name}: {e}\")\n",
        "\n",
        "        # Create candidate profile\n",
        "        candidate = {\n",
        "            'name': candidate_name,\n",
        "            'smiles': smiles,\n",
        "            'molecular_weight': Descriptors.MolWt(mol),\n",
        "            'logp': Descriptors.MolLogP(mol),\n",
        "            'predictions': predictions,\n",
        "            'drug_likeness': self._calculate_drug_likeness(mol),\n",
        "            'diabetes_potential': np.mean(list(predictions.values())) if predictions else 0\n",
        "        }\n",
        "\n",
        "        self.candidates.append(candidate)\n",
        "        return candidate\n",
        "\n",
        "    def generate_optimized_candidate(self, base_smiles, iterations=10):\n",
        "        \"\"\"Generate optimized candidate by modifying structure\"\"\"\n",
        "        print(f\"Optimizing candidate from base SMILES: {base_smiles}\")\n",
        "\n",
        "        best_candidate = None\n",
        "        best_score = -np.inf\n",
        "\n",
        "        for i in range(iterations):\n",
        "            # Generate modified SMILES (simplified - in practice use more sophisticated methods)\n",
        "            modified_smiles = self._mutate_smiles(base_smiles)\n",
        "            candidate = self.generate_candidate_from_smiles(\n",
        "                modified_smiles,\n",
        "                f\"Optimized_Candidate_{i+1}\"\n",
        "            )\n",
        "\n",
        "            if candidate:\n",
        "                score = candidate['diabetes_potential'] * 0.7 + candidate['drug_likeness']['drug_score'] * 0.3\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_candidate = candidate\n",
        "\n",
        "        return best_candidate\n",
        "\n",
        "    def _calculate_all_features(self, mol):\n",
        "        \"\"\"Calculate all features needed for ML models\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Basic descriptors\n",
        "        basic_features = {\n",
        "            'mol_weight': Descriptors.MolWt(mol),\n",
        "            'logp': Descriptors.MolLogP(mol),\n",
        "            'hbd': Descriptors.NumHDonors(mol),\n",
        "            'hba': Descriptors.NumHAcceptors(mol),\n",
        "            'tpsa': Descriptors.TPSA(mol),\n",
        "            'rotatable_bonds': Descriptors.NumRotatableBonds(mol),\n",
        "            'heavy_atoms': mol.GetNumHeavyAtoms(),\n",
        "            'ring_count': Descriptors.RingCount(mol),\n",
        "            'fraction_sp3': Descriptors.FractionCSP3(mol),\n",
        "            'num_aromatic_rings': Descriptors.NumAromaticRings(mol),\n",
        "            'num_saturated_rings': Descriptors.NumSaturatedRings(mol),\n",
        "            'num_heteroatoms': Descriptors.NumHeteroatoms(mol),\n",
        "            'mol_refractivity': Descriptors.MolMR(mol),\n",
        "            'balaban_j': Descriptors.BalabanJ(mol) if mol.GetNumAtoms() > 1 else 0,\n",
        "            'chi0v': Descriptors.Chi0v(mol),\n",
        "            'chi1v': Descriptors.Chi1v(mol),\n",
        "            'kappa1': Descriptors.Kappa1(mol),\n",
        "            'kappa2': Descriptors.Kappa2(mol),\n",
        "            'kappa3': Descriptors.Kappa3(mol),\n",
        "            'lipinski_violations': sum([\n",
        "                1 if Descriptors.MolWt(mol) > 500 else 0,\n",
        "                1 if Descriptors.MolLogP(mol) > 5 else 0,\n",
        "                1 if Descriptors.NumHDonors(mol) > 5 else 0,\n",
        "                1 if Descriptors.NumHAcceptors(mol) > 10 else 0\n",
        "            ])\n",
        "        }\n",
        "\n",
        "        features.update(basic_features)\n",
        "\n",
        "        # No need to add categorical features here, they are handled in generate_candidate_from_smiles\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _calculate_drug_likeness(self, mol):\n",
        "        \"\"\"Calculate comprehensive drug-likeness score\"\"\"\n",
        "        try:\n",
        "            qed = Descriptors.qed(mol)\n",
        "        except:\n",
        "            qed = 0.5\n",
        "\n",
        "        # Lipinski's Rule of Five\n",
        "        lipinski_score = sum([\n",
        "            1 if Descriptors.MolWt(mol) <= 500 else 0,\n",
        "            1 if Descriptors.MolLogP(mol) <= 5 else 0,\n",
        "            1 if Descriptors.NumHDonors(mol) <= 5 else 0,\n",
        "            1 if Descriptors.NumHAcceptors(mol) <= 10 else 0\n",
        "        ]) / 4\n",
        "\n",
        "        # Veber's rules (good oral bioavailability)\n",
        "        rotatable_bonds = Descriptors.NumRotatableBonds(mol)\n",
        "        tpsa = Descriptors.TPSA(mol)\n",
        "        veber_score = 1 if (rotatable_bonds <= 10 and tpsa <= 140) else 0.5\n",
        "\n",
        "        # Combined score\n",
        "        drug_score = (qed * 0.4 + lipinski_score * 0.3 + veber_score * 0.3)\n",
        "\n",
        "        return {\n",
        "            'qed': qed,\n",
        "            'lipinski_compliance': lipinski_score,\n",
        "            'veber_compliance': veber_score,\n",
        "            'drug_score': drug_score,\n",
        "            'is_druglike': drug_score > 0.6\n",
        "        }\n",
        "\n",
        "    def _mutate_smiles(self, smiles):\n",
        "        \"\"\"Simple SMILES mutation (for demonstration)\"\"\"\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if not mol:\n",
        "            return smiles\n",
        "\n",
        "        # Add a methyl group or change a bond (simplified)\n",
        "        from rdkit.Chem import rdMolDescriptors\n",
        "        import random\n",
        "\n",
        "        # Simple mutation: sometimes add a methyl group\n",
        "        if random.random() > 0.5:\n",
        "            # This is simplified - in practice use more sophisticated methods\n",
        "            return smiles + \"C\"  # Just append a carbon\n",
        "\n",
        "        return smiles\n",
        "\n",
        "# ============================================\n",
        "# 6. MAIN PIPELINE WITH MACHINE LEARNING\n",
        "# ============================================\n",
        "\n",
        "def main_ml_pipeline():\n",
        "    \"\"\"Main pipeline with machine learning integration\"\"\"\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"MACHINE LEARNING DRIVEN DRUG DISCOVERY FOR DIABETES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # File path\n",
        "    filepath = \"MedicineOne.csv\"\n",
        "\n",
        "    # Step 1: Data Processing\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 1: DATA PROCESSING\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    processor = DrugDataProcessor(filepath)\n",
        "    processor.df = processor.load_and_clean()\n",
        "\n",
        "    # Extract features\n",
        "    processor.extract_numerical_features()\n",
        "    processor.extract_molecular_features() # This adds mol features to processor.df\n",
        "    processor.encode_categorical_features() # This adds categorical features to processor.df\n",
        "    processor.create_target_variables()\n",
        "\n",
        "    # Prepare ML data\n",
        "    ml_data = processor.prepare_ml_data()\n",
        "\n",
        "    print(f\"\\nProcessed dataset shape: {processor.df.shape}\")\n",
        "    print(f\"Numerical features: {len(processor.numerical_features)}\")\n",
        "    print(f\"Categorical features: {len(processor.categorical_features)}\")\n",
        "    print(f\"Target variables: {processor.target_columns}\")\n",
        "\n",
        "    # Step 2: Machine Learning Modeling\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 2: MACHINE LEARNING MODELING\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    ml_engine = DrugDiscoveryML()\n",
        "    visualizer = DrugDiscoveryVisualizer()\n",
        "\n",
        "    trained_models = {}\n",
        "    ml_results = {}\n",
        "\n",
        "    # Train models for each target\n",
        "    for target_name, data in ml_data.items():\n",
        "        print(f\"\\nTraining models for: {target_name}\")\n",
        "\n",
        "        X = data['X']\n",
        "        y = data['y']\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Determine problem type\n",
        "        problem_type = 'classification' if len(set(y)) < 10 else 'regression'\n",
        "        print(f\"Problem type: {problem_type}\")\n",
        "\n",
        "        # Train multiple models\n",
        "        model_results = {}\n",
        "\n",
        "        if problem_type == 'classification':\n",
        "            # Random Forest\n",
        "            rf_model, rf_metrics = ml_engine.train_classification_model(\n",
        "                X_train, y_train, X_test, y_test, 'rf'\n",
        "            )\n",
        "            trained_models[f'{target_name}_rf'] = rf_model\n",
        "            model_results['Random Forest'] = rf_metrics\n",
        "\n",
        "            # XGBoost\n",
        "            xgb_model, xgb_metrics = ml_engine.train_classification_model(\n",
        "                X_train, y_train, X_test, y_test, 'xgboost'\n",
        "            )\n",
        "            trained_models[f'{target_name}_xgb'] = xgb_model\n",
        "            model_results['XGBoost'] = xgb_metrics\n",
        "\n",
        "            # LightGBM\n",
        "            lgb_model, lgb_metrics = ml_engine.train_classification_model(\n",
        "                X_train, y_train, X_test, y_test, 'lightgbm'\n",
        "            )\n",
        "            trained_models[f'{target_name}_lgb'] = lgb_model\n",
        "            model_results['LightGBM'] = lgb_metrics\n",
        "\n",
        "            # Ensemble\n",
        "            ensemble_model, ensemble_metrics = ml_engine.ensemble_voting(\n",
        "                X_train, y_train, X_test, y_test, 'classification'\n",
        "            )\n",
        "            trained_models[f'{target_name}_ensemble'] = ensemble_model\n",
        "            model_results['Ensemble'] = ensemble_metrics\n",
        "\n",
        "        else:  # Regression\n",
        "            # Random Forest\n",
        "            rf_model, rf_metrics = ml_engine.train_regression_model(\n",
        "                X_train, y_train, X_test, y_test, 'rf'\n",
        "            )\n",
        "            trained_models[f'{target_name}_rf'] = rf_model\n",
        "            model_results['Random Forest'] = rf_metrics\n",
        "\n",
        "            # XGBoost\n",
        "            xgb_model, xgb_metrics = ml_engine.train_regression_model(\n",
        "                X_train, y_train, X_test, y_test, 'xgboost'\n",
        "            )\n",
        "            trained_models[f'{target_name}_xgb'] = xgb_model\n",
        "            model_results['XGBoost'] = xgb_metrics\n",
        "\n",
        "            # LightGBM\n",
        "            lgb_model, lgb_metrics = ml_engine.train_regression_model(\n",
        "                X_train, y_train, X_test, y_test, 'lightgbm'\n",
        "            )\n",
        "            trained_models[f'{target_name}_lgb'] = lgb_model\n",
        "            model_results['LightGBM'] = lgb_metrics\n",
        "\n",
        "        # Deep Learning (if available)\n",
        "        if DEEP_LEARNING_AVAILABLE and len(X_train) > 100:\n",
        "            print(\"Training Deep Learning model...\")\n",
        "            dl_model, dl_metrics, _ = ml_engine.train_deep_learning_model(\n",
        "                X_train, y_train, X_test, y_test, problem_type\n",
        "            )\n",
        "            if dl_model:\n",
        "                trained_models[f'{target_name}_dl'] = dl_model\n",
        "                model_results['Deep Learning'] = dl_metrics\n",
        "\n",
        "        # Store results\n",
        "        ml_results[target_name] = model_results\n",
        "\n",
        "        # Visualize model performance\n",
        "        visualizer.plot_model_performance(model_results,\n",
        "                                        title=f\"Model Performance for {target_name}\")\n",
        "\n",
        "        # Visualize predictions\n",
        "        if problem_type == 'classification':\n",
        "            y_pred = ensemble_model.predict(X_test) if 'ensemble_model' in locals() else rf_model.predict(X_test)\n",
        "        else:\n",
        "            y_pred = rf_model.predict(X_test)\n",
        "\n",
        "        visualizer.plot_prediction_distribution(y_test, y_pred,\n",
        "                                               title=f\"Predictions for {target_name}\")\n",
        "\n",
        "    # Step 3: Feature Importance Analysis\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 3: FEATURE IMPORTANCE ANALYSIS\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    if ml_engine.feature_importance:\n",
        "        visualizer.plot_feature_importance(ml_engine.feature_importance)\n",
        "\n",
        "    # Step 4: Clustering Analysis\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 4: CLUSTERING ANALYSIS\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Use features for clustering\n",
        "    features_for_clustering = processor.numerical_features[:50]  # Use first 50 features\n",
        "    X_cluster = processor.df[features_for_clustering].fillna(0)\n",
        "\n",
        "    # Perform clustering\n",
        "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(X_cluster)\n",
        "\n",
        "    # Add cluster labels to dataframe\n",
        "    processor.df['cluster'] = cluster_labels\n",
        "\n",
        "    # Visualize clusters\n",
        "    visualizer.plot_cluster_analysis(X_cluster, cluster_labels,\n",
        "                                    title=\"Drug Compound Clustering\")\n",
        "\n",
        "    # Analyze clusters\n",
        "    print(\"\\nCluster Analysis:\")\n",
        "    for cluster_id in range(5):\n",
        "        cluster_drugs = processor.df[processor.df['cluster'] == cluster_id]\n",
        "        print(f\"\\nCluster {cluster_id}: {len(cluster_drugs)} drugs\")\n",
        "        if len(cluster_drugs) > 0:\n",
        "            avg_efficacy = cluster_drugs['Efficacy %_numeric'].mean()\n",
        "            avg_bioavailability = cluster_drugs['Bioavailability_numeric'].mean()\n",
        "            diabetes_drugs = cluster_drugs['is_diabetes_drug'].sum()\n",
        "            print(f\"  Avg Efficacy: {avg_efficacy:.1f}%\")\n",
        "            print(f\"  Avg Bioavailability: {avg_bioavailability:.1f}%\")\n",
        "            print(f\"  Diabetes drugs: {diabetes_drugs}\")\n",
        "\n",
        "    # Step 5: Virtual Screening\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 5: VIRTUAL SCREENING\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Pass all feature names to VirtualScreener\n",
        "    all_ml_feature_names = processor.numerical_features + processor.categorical_features\n",
        "    screener = VirtualScreener(trained_models, all_ml_feature_names)\n",
        "\n",
        "    # Test with some hypothetical SMILES\n",
        "    test_smiles = [\n",
        "        \"CC(C)NCC(O)CC1=CC=C(C=C1)OC2=NC=C(C=N2)C3=CC=CC=C3\",  # Our hypothetical candidate\n",
        "        \"CC1=CC=C(C=C1)C(C)(C)CC(C2=CC=C(C=C2)C(=O)O)O\",  # Similar to Atorvastatin\n",
        "        \"CN(C)C(=N)N=C(N)N\",  # Metformin-like\n",
        "        \"CC(C)C[C@H](C(=O)O)NC(=O)[C@H](CC1=CC=CC=C1)NC(=O)[C@H](CC2=CN=CN2)N\",  # Lisinopril-like\n",
        "    ]\n",
        "\n",
        "    screening_results = screener.screen_smiles(test_smiles)\n",
        "\n",
        "    print(\"\\nVirtual Screening Results:\")\n",
        "    for _, result in screening_results.iterrows():\n",
        "        print(f\"\\nSMILES: {result['smiles'][:50]}...\")\n",
        "        print(f\"Drug-likeness Score: {result['drug_likeness']['drug_score']:.3f}\")\n",
        "        if 'predictions' in result and 'ensemble_average' in result['predictions']:\n",
        "            diabetes_potential = result['predictions']['ensemble_average']\n",
        "            if isinstance(diabetes_potential, np.ndarray):\n",
        "                diabetes_potential = diabetes_potential[0]\n",
        "            print(f\"Diabetes Potential Score: {diabetes_potential:.3f}\")\n",
        "\n",
        "    # Step 6: Generate Novel Candidates with ML\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 6: GENERATING NOVEL CANDIDATES WITH ML\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Use diabetes prediction model for candidate generation\n",
        "    diabetes_models = {k: v for k, v in trained_models.items() if 'is_diabetes_drug' in k}\n",
        "\n",
        "    if diabetes_models:\n",
        "        candidate_generator = MLDrivenDrugCandidate(\n",
        "            diabetes_models,\n",
        "            processor.numerical_features + processor.categorical_features\n",
        "        )\n",
        "\n",
        "        # Generate candidate from base SMILES\n",
        "        base_smiles = \"CC(C)NCC(O)CC1=CC=C(C=C1)OC2=NC=C(C=N2)C3=CC=CC=C3\"\n",
        "        novel_candidate = candidate_generator.generate_candidate_from_smiles(\n",
        "            base_smiles, \"ML_Diabetes_Candidate_1\"\n",
        "        )\n",
        "\n",
        "        if novel_candidate:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"ML-GENERATED NOVEL DIABETES DRUG CANDIDATE\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            print(f\"\\nCandidate Name: {novel_candidate['name']}\")\n",
        "            print(f\"SMILES: {novel_candidate['smiles']}\")\n",
        "            print(f\"Molecular Weight: {novel_candidate['molecular_weight']:.2f} Da\")\n",
        "            print(f\"LogP: {novel_candidate['logp']:.2f}\")\n",
        "            print(f\"Drug-likeness Score: {novel_candidate['drug_likeness']['drug_score']:.3f}\")\n",
        "            print(f\"Diabetes Potential Score: {novel_candidate['diabetes_potential']:.3f}\")\n",
        "\n",
        "            print(\"\\nML Model Predictions:\")\n",
        "            for model_name, prediction in novel_candidate['predictions'].items():\n",
        "                if '_probability' not in model_name:\n",
        "                    print(f\"  {model_name}: {prediction:.3f}\")\n",
        "\n",
        "            print(\"\\nDrug-likeness Analysis:\")\n",
        "            for key, value in novel_candidate['drug_likeness'].items():\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"  {key}: {value:.3f}\")\n",
        "                else:\n",
        "                    print(f\"  {key}: {value}\")\n",
        "\n",
        "        # Try optimization\n",
        "        print(\"\\n\" + \"-\" * 40)\n",
        "        print(\"Attempting candidate optimization...\")\n",
        "        optimized_candidate = candidate_generator.generate_optimized_candidate(base_smiles, iterations=5)\n",
        "\n",
        "        if optimized_candidate:\n",
        "            print(f\"\\nOptimized Candidate: {optimized_candidate['name']}\")\n",
        "            print(f\"Optimized SMILES: {optimized_candidate['smiles']}\")\n",
        "            print(f\"Optimized Diabetes Potential: {optimized_candidate['diabetes_potential']:.3f}\")\n",
        "            print(f\"Optimized Drug-likeness: {optimized_candidate['drug_likeness']['drug_score']:.3f}\")\n",
        "\n",
        "    # Step 7: Generate Comprehensive Report\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 7: GENERATING COMPREHENSIVE REPORT\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Save results\n",
        "    with open('ml_drug_discovery_report.txt', 'w') as f:\n",
        "        f.write(\"=\" * 70 + \"\\n\")\n",
        "        f.write(\"MACHINE LEARNING DRUG DISCOVERY REPORT - DIABETES\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"1. DATASET SUMMARY\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        f.write(f\"Total drugs in dataset: {len(processor.df)}\\n\")\n",
        "        f.write(f\"Diabetes drugs identified: {processor.df['is_diabetes_drug'].sum()}\\n\")\n",
        "        f.write(f\"High efficacy drugs (>80%): {processor.df['high_efficacy'].sum()}\\n\\n\")\n",
        "\n",
        "        f.write(\"2. MACHINE LEARNING PERFORMANCE\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        for target_name, model_results in ml_results.items():\n",
        "            f.write(f\"\\nTarget: {target_name}\\n\")\n",
        "            for model_name, metrics in model_results.items():\n",
        "                f.write(f\"  {model_name}:\\n\")\n",
        "                for metric_name, metric_value in metrics.items():\n",
        "                    if metric_name != 'confusion_matrix':\n",
        "                        f.write(f\"    {metric_name}: {metric_value:.4f}\\n\")\n",
        "\n",
        "        f.write(\"\\n3. CLUSTERING ANALYSIS\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        for cluster_id in range(5):\n",
        "            cluster_drugs = processor.df[processor.df['cluster'] == cluster_id]\n",
        "            diabetes_count = cluster_drugs['is_diabetes_drug'].sum()\n",
        "            f.write(f\"\\nCluster {cluster_id}: {len(cluster_drugs)} drugs\")\n",
        "            f.write(f\" (Diabetes drugs: {diabetes_count})\\n\")\n",
        "\n",
        "        f.write(\"\\n4. VIRTUAL SCREENING RESULTS\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        for _, result in screening_results.iterrows():\n",
        "            f.write(f\"\\nSMILES: {result['smiles']}\\n\")\n",
        "            f.write(f\"  Drug Score: {result['drug_likeness']['drug_score']:.3f}\\n\")\n",
        "            if 'predictions' in result and 'ensemble_average' in result['predictions']:\n",
        "                diabetes_potential = result['predictions']['ensemble_average']\n",
        "                if isinstance(diabetes_potential, np.ndarray):\n",
        "                    diabetes_potential = diabetes_potential[0]\n",
        "                f.write(f\"  Diabetes Potential: {diabetes_potential:.3f}\\n\")\n",
        "\n",
        "        f.write(\"\\n5. NOVEL CANDIDATE GENERATION\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        if 'novel_candidate' in locals():\n",
        "            f.write(f\"\\nCandidate: {novel_candidate['name']}\\n\")\n",
        "            f.write(f\"SMILES: {novel_candidate['smiles']}\\n\")\n",
        "            f.write(f\"Molecular Weight: {novel_candidate['molecular_weight']:.2f} Da\\n\")\n",
        "            f.write(f\"LogP: {novel_candidate['logp']:.2f}\\n\")\n",
        "            f.write(f\"Diabetes Potential: {novel_candidate['diabetes_potential']:.3f}\\n\")\n",
        "            f.write(f\"Drug-likeness: {novel_candidate['drug_likeness']['drug_score']:.3f}\\n\")\n",
        "\n",
        "        f.write(\"\\n6. RECOMMENDATIONS\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        f.write(\"1. Prioritize compounds with high ML-predicted diabetes potential\\n\")\n",
        "        f.write(\"2. Focus on clusters with existing diabetes drugs\\n\")\n",
        "        f.write(\"3. Optimize drug-likeness properties while maintaining efficacy\\n\")\n",
        "        f.write(\"4. Consider dual/triple mechanisms for better outcomes\\n\")\n",
        "        f.write(\"5. Validate top candidates with molecular docking studies\\n\")\n",
        "\n",
        "    # Step 8: Create Summary Table\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SUMMARY OF ML-GENERATED CANDIDATES\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    summary_data = []\n",
        "    if 'candidate_generator' in locals():\n",
        "        for candidate in candidate_generator.candidates:\n",
        "            summary_data.append({\n",
        "                'Name': candidate['name'],\n",
        "                'MW': f\"{candidate['molecular_weight']:.1f}\",\n",
        "                'LogP': f\"{candidate['logp']:.2f}\",\n",
        "                'Drug Score': f\"{candidate['drug_likeness']['drug_score']:.3f}\",\n",
        "                'Diabetes Potential': f\"{candidate['diabetes_potential']:.3f}\",\n",
        "                'QED': f\"{candidate['drug_likeness']['qed']:.3f}\",\n",
        "                'Lipinski OK': candidate['drug_likeness']['lipinski_compliance'] > 0.75\n",
        "            })\n",
        "\n",
        "    if summary_data:\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        print(summary_df.to_string(index=False))\n",
        "\n",
        "        # Save summary to CSV\n",
        "        summary_df.to_csv('ml_generated_candidates.csv', index=False)\n",
        "        print(\"\\nSummary saved to 'ml_generated_candidates.csv'\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"\\nGenerated Files:\")\n",
        "    print(\"1. model_performance.png - Model comparison plots\")\n",
        "    print(\"2. feature_importance.png - Feature importance analysis\")\n",
        "    print(\"3. cluster_analysis.png - Clustering visualization\")\n",
        "    print(\"4. prediction_distribution.png - Prediction analysis\")\n",
        "    print(\"5. ml_drug_discovery_report.txt - Comprehensive report\")\n",
        "    print(\"6. ml_generated_candidates.csv - ML-generated candidates\")\n",
        "\n",
        "    return {\n",
        "        'dataframe': processor.df,\n",
        "        'trained_models': trained_models,\n",
        "        'ml_results': ml_results,\n",
        "        'candidates': candidate_generator.candidates if 'candidate_generator' in locals() else []\n",
        "    }\n",
        "\n",
        "# ============================================\n",
        "# 7. EXECUTION\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check for required libraries\n",
        "    required_libraries = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'sklearn', 'rdkit']\n",
        "\n",
        "    missing_libs = []\n",
        "    for lib in required_libraries:\n",
        "        try:\n",
        "            __import__(lib)\n",
        "        except ImportError:\n",
        "            missing_libs.append(lib)\n",
        "\n",
        "    if missing_libs:\n",
        "        print(f\"Missing libraries: {', '.join(missing_libs)}\")\n",
        "        print(\"Please install them using:\")\n",
        "        print(\"pip install pandas numpy matplotlib seaborn scikit-learn rdkit-pypi xgboost lightgbm\")\n",
        "\n",
        "        # Try to run with available libraries\n",
        "        print(\"\\nAttempting to run with available libraries...\")\n",
        "        try:\n",
        "            results = main_ml_pipeline()\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            print(\"\\nPlease install all required libraries for full functionality.\")\n",
        "    else:\n",
        "        # Run the complete pipeline\n",
        "        results = main_ml_pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'results' dictionary from the previous run is available.\n",
        "\n",
        "print(\"--- Comparing Metformin and ML_Diabetes_Candidate_1 ---\")\n",
        "\n",
        "# --- Get data for ML_Diabetes_Candidate_1 ---\n",
        "ml_candidate_info = None\n",
        "for candidate in results['candidates']:\n",
        "    if candidate['name'] == 'ML_Diabetes_Candidate_1':\n",
        "        ml_candidate_info = candidate\n",
        "        break\n",
        "\n",
        "if ml_candidate_info:\n",
        "    ml_cand_smiles = ml_candidate_info['smiles']\n",
        "    ml_cand_drug_likeness = ml_candidate_info['drug_likeness']['drug_score']\n",
        "    # Explicitly get the ensemble prediction for is_diabetes_drug classification\n",
        "    ml_cand_diabetes_potential_ensemble = ml_candidate_info['predictions'].get('is_diabetes_drug_ensemble', np.nan)\n",
        "    if isinstance(ml_cand_diabetes_potential_ensemble, np.ndarray):\n",
        "        ml_cand_diabetes_potential_ensemble = ml_cand_diabetes_potential_ensemble.item()\n",
        "\n",
        "    print(\"\\nML_Diabetes_Candidate_1 Details:\")\n",
        "    print(f\"  SMILES: {ml_cand_smiles}\")\n",
        "    print(f\"  Drug-likeness Score: {ml_cand_drug_likeness:.3f}\")\n",
        "    print(f\"  Diabetes Potential (is_diabetes_drug_ensemble): {ml_cand_diabetes_potential_ensemble:.3f}\")\n",
        "else:\n",
        "    print(\"Error: ML_Diabetes_Candidate_1 not found in the generated candidates.\")\n",
        "    ml_cand_smiles, ml_cand_drug_likeness, ml_cand_diabetes_potential_ensemble = \"N/A\", np.nan, np.nan\n",
        "\n",
        "\n",
        "# --- Get data for Metformin-like compound ---\n",
        "metformin_smiles = \"CN(C)C(=N)N=C(N)N\" # Metformin SMILES\n",
        "\n",
        "# Re-initialize the VirtualScreener with trained models and feature names\n",
        "trained_models = results['trained_models']\n",
        "# Extract feature names from one of the trained models\n",
        "# (assuming all models in trained_models were trained on the same feature set and have 'feature_names_in_')\n",
        "try:\n",
        "    all_ml_feature_names = trained_models[list(trained_models.keys())[0]].feature_names_in_.tolist()\n",
        "except AttributeError:\n",
        "    # Fallback if feature_names_in_ is not available or if the model type does not support it\n",
        "    # This part needs to be robust if the original 'processor' object is not available.\n",
        "    # For simplicity, we assume ml_data structure from results can give feature names\n",
        "    print(\"Warning: Could not get feature names directly from a trained model. Inferring from ml_data.\")\n",
        "    first_target_name = list(results['ml_results'].keys())[0]\n",
        "    X_sample = results['ml_results'][first_target_name]['X']\n",
        "    all_ml_feature_names = X_sample.columns.tolist()\n",
        "\n",
        "# Create a new screener instance\n",
        "screener = VirtualScreener(trained_models, all_ml_feature_names)\n",
        "\n",
        "# Screen Metformin SMILES\n",
        "metformin_screening_results_df = screener.screen_smiles([metformin_smiles])\n",
        "\n",
        "metformin_info = metformin_screening_results_df.iloc[0]\n",
        "metformin_drug_likeness = metformin_info['drug_likeness']['drug_score']\n",
        "metformin_diabetes_potential_ensemble = metformin_info['predictions'].get('is_diabetes_drug_ensemble', np.nan)\n",
        "if isinstance(metformin_diabetes_potential_ensemble, np.ndarray):\n",
        "    metformin_diabetes_potential_ensemble = metformin_diabetes_potential_ensemble.item()\n",
        "\n",
        "print(\"\\nMetformin Details (from Virtual Screening):\")\n",
        "print(f\"  SMILES: {metformin_smiles}\")\n",
        "print(f\"  Drug-likeness Score: {metformin_drug_likeness:.3f}\")\n",
        "print(f\"  Diabetes Potential (is_diabetes_drug_ensemble): {metformin_diabetes_potential_ensemble:.3f}\")\n",
        "\n",
        "\n",
        "# --- Final Comparison and Conclusion ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"             FINAL DRUG CANDIDATE COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "print(f\"{'Metric':<30} | {'Metformin':<15} | {'ML_Diabetes_Candidate_1':<25}\")\n",
        "print(\"-\" * 75)\n",
        "print(f\"{'Drug-likeness Score':<30} | {metformin_drug_likeness:<15.3f} | {ml_cand_drug_likeness:<25.3f}\")\n",
        "print(f\"{'Diabetes Potential (Ensemble)':<30} | {metformin_diabetes_potential_ensemble:<15.3f} | {ml_cand_diabetes_potential_ensemble:<25.3f}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "if ml_cand_diabetes_potential_ensemble > metformin_diabetes_potential_ensemble and ml_cand_drug_likeness > metformin_drug_likeness:\n",
        "    print(\"\\nConclusion: ML_Diabetes_Candidate_1 shows promising results, surpassing Metformin in both predicted diabetes potential (classification) and drug-likeness score. It represents a potentially superior candidate.\")\n",
        "elif ml_cand_diabetes_potential_ensemble > metformin_diabetes_potential_ensemble:\n",
        "    print(\"\\nConclusion: ML_Diabetes_Candidate_1 has a higher predicted diabetes potential (classification) than Metformin, but Metformin has a higher drug-likeness score. Its efficacy for diabetes is higher but there is a trade-off in overall drug-like properties compared to Metformin.\")\n",
        "elif metformin_diabetes_potential_ensemble > ml_cand_diabetes_potential_ensemble:\n",
        "    print(\"\\nConclusion: Metformin has a higher predicted diabetes potential (classification) based on the ensemble model. While ML_Diabetes_Candidate_1 has a higher drug-likeness score, its diabetes potential is lower. This suggests Metformin is a better choice for diabetes efficacy according to the model, while the ML candidate might be generally more 'drug-like' overall.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: Based on the current model predictions for diabetes potential (classification) and drug-likeness, the two compounds show mixed results. Further analysis or weighting of criteria would be needed to definitively determine which is 'best'.\")\n",
        "\n",
        "print(\"=\"*50)\n"
      ],
      "metadata": {
        "id": "fLbyDym-i-o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The comparison was already run in the previous step.\n",
        "# Here are the results:\n",
        "\n",
        "print(\"\\n--- Summary of Metformin vs ML_Diabetes_Candidate_1 ---\")\n",
        "print(f\"ML_Diabetes_Candidate_1 Drug-likeness Score: {ml_cand_drug_likeness:.3f}\")\n",
        "print(f\"ML_Diabetes_Candidate_1 Diabetes Potential (Ensemble): {ml_cand_diabetes_potential_ensemble:.3f}\")\n",
        "print(f\"Metformin Drug-likeness Score: {metformin_drug_likeness:.3f}\")\n",
        "print(f\"Metformin Diabetes Potential (Ensemble): {metformin_diabetes_potential_ensemble:.3f}\")\n",
        "\n",
        "print(\"\\nConclusion: Based on these results, ML_Diabetes_Candidate_1 has a higher predicted drug-likeness score. However, both compounds have a very low predicted diabetes potential from the ensemble model. Further investigation or different models might be needed to confirm diabetes efficacy for both compounds.\")"
      ],
      "metadata": {
        "id": "1y7yi-WXjuAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHrTNr4H0hlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Thded1qp1m0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6c9aa04"
      },
      "source": [
        "print('Gradient Boosting Accuracies:')\n",
        "for target, accuracies in gradient_boosting_accuracies.items():\n",
        "    print(f\"Target: {target}\")\n",
        "    for model, accuracy in accuracies.items():\n",
        "        print(f\"  {model}: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8410f99"
      },
      "source": [
        "# Task\n",
        "## Retrieve gradient boosting accuracies\n",
        "\n",
        "I'll access the `ml_results` variable from the previous execution, then retrieve the accuracy metrics for the XGBoost and LightGBM models for all classification tasks.\n",
        "\n",
        "## Display accuracies\n",
        "\n",
        "I'll present the extracted accuracy scores in a clear and readable format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80e5dd45"
      },
      "source": [
        "## Retrieve gradient boosting accuracies\n",
        "\n",
        "### Subtask:\n",
        "Access the `ml_results` variable and extract the accuracy metrics for XGBoost and LightGBM models for all classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f1ae49d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires extracting accuracy metrics for XGBoost and LightGBM models from the `ml_results` dictionary for classification tasks. I will iterate through `ml_results`, identify classification tasks, and store the relevant accuracies in a new dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ad23def"
      },
      "source": [
        "gradient_boosting_accuracies = {}\n",
        "\n",
        "for target, models_results in results['ml_results'].items():\n",
        "    # Check if 'accuracy' metric exists for any model to identify classification tasks\n",
        "    if any('accuracy' in metrics for metrics in models_results.values()):\n",
        "        target_accuracies = {}\n",
        "        if 'XGBoost' in models_results:\n",
        "            target_accuracies['XGBoost'] = models_results['XGBoost'].get('accuracy')\n",
        "        if 'LightGBM' in models_results:\n",
        "            target_accuracies['LightGBM'] = models_results['LightGBM'].get('accuracy')\n",
        "\n",
        "        if target_accuracies:\n",
        "            gradient_boosting_accuracies[target] = target_accuracies\n",
        "\n",
        "print(\"Extracted Gradient Boosting Accuracies:\")\n",
        "for target, accuracies in gradient_boosting_accuracies.items():\n",
        "    print(f\"Target: {target}\")\n",
        "    for model, accuracy in accuracies.items():\n",
        "        print(f\"  {model}: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12dabf55"
      },
      "source": [
        "## Display accuracies\n",
        "\n",
        "### Subtask:\n",
        "Present the extracted accuracy scores in a clear and readable format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4e46288"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `ml_results` variable was successfully processed to extract accuracy metrics for gradient boosting models (XGBoost and LightGBM) for classification tasks.\n",
        "*   For the 'is_diabetes_drug' target, both XGBoost and LightGBM models achieved an accuracy of 0.9405.\n",
        "*   For the 'high_efficacy' target, both XGBoost and LightGBM models achieved a perfect accuracy of 1.0000.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The models show consistent high performance across both gradient boosting algorithms for the given classification tasks, with particularly outstanding results for 'high_efficacy'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c586c3d0"
      },
      "source": [
        "# Task\n",
        "Retrieve and display the confusion matrices for all classification models found within the `ml_results` variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1949d9b8"
      },
      "source": [
        "## Retrieve and display confusion matrices\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the `ml_results` to find all classification models and print their respective confusion matrices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc89bcf5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   No data analysis was performed in this step as the solving process, which typically includes code execution and output, was not provided. The defined task is to retrieve and display confusion matrices specifically for classification models found within the `ml_results` variable.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The immediate next step is to implement and execute the necessary code to iterate through `ml_results`, identify classification models, and generate their respective confusion matrices. This execution will produce the data required for analysis and findings.\n"
      ]
    }
  ]
}