{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUAAVdKikN+3JLPi7TgOXA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikramulhossain/Aiyourvadik/blob/main/final_novel_drug_candidate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy matplotlib seaborn rdkit-pypi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl2182AILRcV",
        "outputId": "56a68e38-04ff-492f-c97e-9bd27adc8877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement rdkit-pypi (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for rdkit-pypi\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgmVlws9LAFm",
        "outputId": "f162366f-891f-4258-a329-3c67de702cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "MACHINE LEARNING DRIVEN DRUG DISCOVERY FOR DIABETES\n",
            "======================================================================\n",
            "\n",
            "========================================\n",
            "STEP 1: DATA PROCESSING\n",
            "========================================\n",
            "Loading dataset...\n",
            "Dataset shape: (421, 27)\n",
            "Missing values per column:\n",
            "medicineNname                            0\n",
            "ActiveIngredient                         0\n",
            "SMILES                                   0\n",
            "Target Protein / Enzyme                  0\n",
            "Protein Binding Affinity (Kd/IC50/Ki)    0\n",
            "Efficacy %                               0\n",
            "Toxicity                                 0\n",
            "Mechanism of Action                      0\n",
            "Absorption                               0\n",
            "Distribution                             0\n",
            "Metabolism                               0\n",
            "Excretion                                0\n",
            "Bioavailability                          0\n",
            "Bioavailability/Key_Notes                0\n",
            "Stability                                0\n",
            "Dose_Range                               0\n",
            "Selectivity                              0\n",
            "Potency                                  0\n",
            "Agonist/Antagonist Activity              0\n",
            "Side Effects                             0\n",
            "Drug Interactions                        0\n",
            "Solubility                               0\n",
            "Lipophilicity - LogP                     0\n",
            "pKa                                      1\n",
            "Molecular Weight                         0\n",
            "Manufacturability                        0\n",
            "Patentability                            0\n",
            "dtype: int64\n",
            "\n",
            "Extracting molecular features from SMILES...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[21:42:52] SMILES Parse Error: syntax error while parsing: CC(C)CC@HNC(=O)C@HNC(=O)C@HN\n",
            "[21:42:52] SMILES Parse Error: check for mistakes around position 8:\n",
            "[21:42:52] CC(C)CC@HNC(=O)C@HNC(=O)C@HN\n",
            "[21:42:52] ~~~~~~~^\n",
            "[21:42:52] SMILES Parse Error: Failed parsing SMILES 'CC(C)CC@HNC(=O)C@HNC(=O)C@HN' for input: 'CC(C)CC@HNC(=O)C@HNC(=O)C@HN'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 20 molecular features\n",
            "Created target variables: ['is_diabetes_drug', 'high_efficacy', 'Bioavailability_numeric']\n",
            "\n",
            "Processed dataset shape: (406, 96)\n",
            "Numerical features: 27\n",
            "Categorical features: 40\n",
            "Target variables: ['is_diabetes_drug', 'high_efficacy', 'Bioavailability_numeric']\n",
            "\n",
            "========================================\n",
            "STEP 2: MACHINE LEARNING MODELING\n",
            "========================================\n",
            "\n",
            "Training models for: is_diabetes_drug\n",
            "Problem type: classification\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 38, number of negative: 286\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000131 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1325\n",
            "[LightGBM] [Info] Number of data points in the train set: 324, number of used features: 36\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117284 -> initscore=-2.018406\n",
            "[LightGBM] [Info] Start training from score -2.018406\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 38, number of negative: 286\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1325\n",
            "[LightGBM] [Info] Number of data points in the train set: 324, number of used features: 36\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117284 -> initscore=-2.018406\n",
            "[LightGBM] [Info] Start training from score -2.018406\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training Deep Learning model...\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\n",
            "Training models for: high_efficacy\n",
            "Problem type: classification\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 88, number of negative: 236\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1326\n",
            "[LightGBM] [Info] Number of data points in the train set: 324, number of used features: 37\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.271605 -> initscore=-0.986495\n",
            "[LightGBM] [Info] Start training from score -0.986495\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 88, number of negative: 236\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000311 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1326\n",
            "[LightGBM] [Info] Number of data points in the train set: 324, number of used features: 37\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.271605 -> initscore=-0.986495\n",
            "[LightGBM] [Info] Start training from score -0.986495\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training Deep Learning model...\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\n",
            "Training models for: Bioavailability_numeric\n",
            "Problem type: regression\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000140 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1331\n",
            "[LightGBM] [Info] Number of data points in the train set: 324, number of used features: 37\n",
            "[LightGBM] [Info] Start training from score 60.004012\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training Deep Learning model...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys # Added for sys.modules check\n",
        "\n",
        "# Attempt to install rdkit if not already present and running in Colab\n",
        "if 'google.colab' in sys.modules and 'rdkit' not in sys.modules:\n",
        "    print(\"RDKit not found, attempting installation with 'pip install rdkit'...\")\n",
        "    !pip install rdkit\n",
        "    print(\"RDKit installation initiated. A runtime restart (Runtime -> Restart runtime) is strongly recommended for RDKit to be fully functional.\")\n",
        "    print(\"Proceeding without restart for this execution, but if errors persist, please restart.\")\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Lipinski, PandasTools, Draw\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import DataStructs\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                            roc_auc_score, confusion_matrix, classification_report,\n",
        "                            mean_squared_error, r2_score, mean_absolute_error)\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Deep Learning Libraries\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers, models\n",
        "    DEEP_LEARNING_AVAILABLE = True\n",
        "except:\n",
        "    DEEP_LEARNING_AVAILABLE = False\n",
        "    print(\"TensorFlow not available. Deep learning models will be skipped.\")\n",
        "\n",
        "# ============================================\n",
        "# 1. DATA LOADING AND PREPROCESSING WITH ML\n",
        "# ============================================\n",
        "\n",
        "class DrugDataProcessor:\n",
        "    \"\"\"Process drug dataset and prepare for ML models\"\"\"\n",
        "\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.df = None\n",
        "        self.numerical_features = []\n",
        "        self.categorical_features = []\n",
        "        self.target_columns = []\n",
        "\n",
        "    def load_and_clean(self):\n",
        "        \"\"\"Load and clean the dataset\"\"\"\n",
        "        print(\"Loading dataset...\")\n",
        "        # Added encoding='latin1' to handle potential UnicodeDecodeError\n",
        "        self.df = pd.read_csv(self.filepath)\n",
        "\n",
        "        # Clean column names\n",
        "        self.df.columns = self.df.columns.str.strip().str.replace('ï»¿', '', regex=False)\n",
        "\n",
        "        # Basic info\n",
        "        print(f\"Dataset shape: {self.df.shape}\")\n",
        "        print(f\"Missing values per column:\")\n",
        "        print(self.df.isnull().sum())\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def extract_numerical_features(self):\n",
        "        \"\"\"Extract numerical features from the dataset\"\"\"\n",
        "        numerical_data = {}\n",
        "\n",
        "        # Extract numerical values from string columns\n",
        "        columns_to_extract = [\n",
        "            'Efficacy %', 'Bioavailability', 'Molecular Weight',\n",
        "            'Lipophilicity - LogP', 'pKa'\n",
        "        ]\n",
        "\n",
        "        for col in columns_to_extract:\n",
        "            if col in self.df.columns:\n",
        "                # Extract first numerical value from string\n",
        "                self.df[f'{col}_numeric'] = self.df[col].astype(str).str.extract('([-+]?\\d*\\.\\d+|\\d+)')[0]\n",
        "                self.df[f'{col}_numeric'] = pd.to_numeric(self.df[f'{col}_numeric'], errors='coerce')\n",
        "                numerical_data[col] = self.df[f'{col}_numeric']\n",
        "                self.numerical_features.append(f'{col}_numeric')\n",
        "\n",
        "        # Extract Toxicity score (simplified)\n",
        "        def toxicity_to_score(toxicity):\n",
        "            if pd.isna(toxicity):\n",
        "                return 2  # Medium\n",
        "            toxicity = str(toxicity).lower()\n",
        "            if 'high' in toxicity or 'severe' in toxicity:\n",
        "                return 3\n",
        "            elif 'moderate' in toxicity:\n",
        "                return 2\n",
        "            elif 'low' in toxicity or 'mild' in toxicity:\n",
        "                return 1\n",
        "            else:\n",
        "                return 2\n",
        "\n",
        "        self.df['toxicity_score'] = self.df['Toxicity'].apply(toxicity_to_score)\n",
        "        self.numerical_features.append('toxicity_score')\n",
        "\n",
        "        # Extract Patentability (binary)\n",
        "        self.df['is_patented'] = self.df['Patentability'].apply(\n",
        "            lambda x: 1 if 'brand' in str(x).lower() or 'still' in str(x).lower() else 0\n",
        "        )\n",
        "        self.numerical_features.append('is_patented')\n",
        "\n",
        "        return numerical_data\n",
        "\n",
        "    def extract_molecular_features(self, smiles_column='SMILES'):\n",
        "        \"\"\"Extract molecular descriptors from SMILES using RDKit\"\"\"\n",
        "        print(\"\\nExtracting molecular features from SMILES...\")\n",
        "\n",
        "        molecular_features = []\n",
        "\n",
        "        for idx, row in self.df.iterrows():\n",
        "            if pd.isna(row[smiles_column]):\n",
        "                features = {f'mol_feat_{i}': np.nan for i in range(20)}\n",
        "            else:\n",
        "                mol = Chem.MolFromSmiles(str(row[smiles_column]))\n",
        "                if mol:\n",
        "                    # Basic molecular descriptors\n",
        "                    features = {\n",
        "                        'mol_weight': Descriptors.MolWt(mol),\n",
        "                        'logp': Descriptors.MolLogP(mol),\n",
        "                        'hbd': Descriptors.NumHDonors(mol),\n",
        "                        'hba': Descriptors.NumHAcceptors(mol),\n",
        "                        'tpsa': Descriptors.TPSA(mol),\n",
        "                        'rotatable_bonds': Descriptors.NumRotatableBonds(mol),\n",
        "                        'heavy_atoms': mol.GetNumHeavyAtoms(),\n",
        "                        'ring_count': Descriptors.RingCount(mol),\n",
        "                        'fraction_sp3': Descriptors.FractionCSP3(mol),\n",
        "                        'num_aromatic_rings': Descriptors.NumAromaticRings(mol),\n",
        "                        'num_saturated_rings': Descriptors.NumSaturatedRings(mol),\n",
        "                        'num_heteroatoms': Descriptors.NumHeteroatoms(mol),\n",
        "                        'mol_refractivity': Descriptors.MolMR(mol),\n",
        "                        'balaban_j': Descriptors.BalabanJ(mol) if mol.GetNumAtoms() > 1 else 0,\n",
        "                        'chi0v': Descriptors.Chi0v(mol),\n",
        "                        'chi1v': Descriptors.Chi1v(mol),\n",
        "                        'kappa1': Descriptors.Kappa1(mol),\n",
        "                        'kappa2': Descriptors.Kappa2(mol),\n",
        "                        'kappa3': Descriptors.Kappa3(mol),\n",
        "                        'lipinski_violations': sum([\n",
        "                            1 if Descriptors.MolWt(mol) > 500 else 0,\n",
        "                            1 if Descriptors.MolLogP(mol) > 5 else 0,\n",
        "                            1 if Descriptors.NumHDonors(mol) > 5 else 0,\n",
        "                            1 if Descriptors.NumHAcceptors(mol) > 10 else 0\n",
        "                        ])\n",
        "                    }\n",
        "                else:\n",
        "                    features = {k: np.nan for k in [\n",
        "                        'mol_weight', 'logp', 'hbd', 'hba', 'tpsa',\n",
        "                        'rotatable_bonds', 'heavy_atoms', 'ring_count',\n",
        "                        'fraction_sp3', 'num_aromatic_rings', 'num_saturated_rings',\n",
        "                        'num_heteroatoms', 'mol_refractivity', 'balaban_j',\n",
        "                        'chi0v', 'chi1v', 'kappa1', 'kappa2', 'kappa3',\n",
        "                        'lipinski_violations'\n",
        "                    ]}\n",
        "\n",
        "            molecular_features.append(features)\n",
        "\n",
        "        # Add to dataframe\n",
        "        mol_df = pd.DataFrame(molecular_features)\n",
        "        for col in mol_df.columns:\n",
        "            self.df[col] = mol_df[col]\n",
        "            self.numerical_features.append(col)\n",
        "\n",
        "        print(f\"Added {len(mol_df.columns)} molecular features\")\n",
        "\n",
        "        return mol_df\n",
        "\n",
        "    def encode_categorical_features(self):\n",
        "        \"\"\"Encode categorical features for ML\"\"\"\n",
        "        categorical_cols = [\n",
        "            'Mechanism of Action', 'Agonist/Antagonist Activity',\n",
        "            'Selectivity', 'Manufacturability'\n",
        "        ]\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            if col in self.df.columns:\n",
        "                # Create dummy variables for top categories\n",
        "                top_categories = self.df[col].value_counts().head(10).index\n",
        "                self.df[col] = self.df[col].apply(\n",
        "                    lambda x: x if x in top_categories else 'Other'\n",
        "                )\n",
        "                dummies = pd.get_dummies(self.df[col], prefix=col, drop_first=True)\n",
        "                self.df = pd.concat([self.df, dummies], axis=1)\n",
        "                self.categorical_features.extend(dummies.columns.tolist())\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def create_target_variables(self):\n",
        "        \"\"\"Create target variables for ML models\"\"\"\n",
        "        # Target 1: Diabetes Drug Classification\n",
        "        diabetes_keywords = [\n",
        "            'insulin', 'glucose', 'diabetes', 'glp', 'gip', 'sglt', 'dpp',\n",
        "            'sulfonylurea', 'metformin', 'ppar', 'glucagon', 'incretin','ACC','ACE'\n",
        "        ]\n",
        "\n",
        "        def is_diabetes_related(text):\n",
        "            if pd.isna(text):\n",
        "                return 0\n",
        "            text = str(text).lower()\n",
        "            return 1 if any(keyword in text for keyword in diabetes_keywords) else 0\n",
        "\n",
        "        self.df['is_diabetes_drug'] = self.df['Target Protein / Enzyme'].apply(is_diabetes_related)\n",
        "        self.target_columns.append('is_diabetes_drug')\n",
        "\n",
        "        # Target 2: High Efficacy (binary classification)\n",
        "        self.df['high_efficacy'] = self.df['Efficacy %_numeric'].apply(\n",
        "            lambda x: 1 if x > 80 else 0 if pd.notna(x) else np.nan\n",
        "        )\n",
        "        self.target_columns.append('high_efficacy')\n",
        "\n",
        "        # Target 3: Bioavailability (regression)\n",
        "        self.target_columns.append('Bioavailability_numeric')\n",
        "\n",
        "        print(f\"Created target variables: {self.target_columns}\")\n",
        "\n",
        "        return self.target_columns\n",
        "\n",
        "    def prepare_ml_data(self):\n",
        "        \"\"\"Prepare clean dataset for ML\"\"\"\n",
        "        # Remove rows with too many missing values\n",
        "        self.df = self.df.dropna(subset=self.target_columns)\n",
        "\n",
        "        # Fill missing numerical values with median\n",
        "        for col in self.numerical_features:\n",
        "            if col in self.df.columns:\n",
        "                self.df[col] = self.df[col].fillna(self.df[col].median())\n",
        "\n",
        "        # Prepare feature matrix X and target y\n",
        "        features = self.numerical_features + self.categorical_features\n",
        "        X = self.df[features]\n",
        "\n",
        "        ml_data = {}\n",
        "        for target in self.target_columns:\n",
        "            if target in self.df.columns:\n",
        "                y = self.df[target]\n",
        "                # Remove rows where target is NaN\n",
        "                mask = y.notna()\n",
        "                ml_data[target] = {\n",
        "                    'X': X[mask],\n",
        "                    'y': y[mask]\n",
        "                }\n",
        "\n",
        "        return ml_data\n",
        "\n",
        "# ============================================\n",
        "# 2. MACHINE LEARNING MODELS FOR DRUG DISCOVERY\n",
        "# ============================================\n",
        "\n",
        "class DrugDiscoveryML:\n",
        "    \"\"\"Machine Learning models for drug discovery\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.feature_importance = {}\n",
        "\n",
        "    def train_classification_model(self, X_train, y_train, X_test, y_test, model_type='rf'):\n",
        "        \"\"\"Train classification model\"\"\"\n",
        "        if model_type == 'rf':\n",
        "            model = RandomForestClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=10,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                random_state=42\n",
        "            )\n",
        "        elif model_type == 'svm':\n",
        "            model = SVC(probability=True, random_state=42)\n",
        "        elif model_type == 'logistic':\n",
        "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "        elif model_type == 'xgboost':\n",
        "            model = xgb.XGBClassifier(random_state=42)\n",
        "        elif model_type == 'lightgbm':\n",
        "            model = lgb.LGBMClassifier(random_state=42)\n",
        "        else:\n",
        "            model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_test, y_pred),\n",
        "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "            'f1': f1_score(y_test, y_pred, zero_division=0),\n",
        "            'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
        "        }\n",
        "\n",
        "        if y_pred_proba is not None and len(set(y_test)) > 1:\n",
        "            metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "        # Feature importance\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            self.feature_importance[model_type] = dict(zip(X_train.columns, model.feature_importances_))\n",
        "\n",
        "        return model, metrics\n",
        "\n",
        "    def train_regression_model(self, X_train, y_train, X_test, y_test, model_type='rf'):\n",
        "        \"\"\"Train regression model\"\"\"\n",
        "        if model_type == 'rf':\n",
        "            model = RandomForestRegressor(\n",
        "                n_estimators=100,\n",
        "                max_depth=10,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                random_state=42\n",
        "            )\n",
        "        elif model_type == 'svm':\n",
        "            model = SVR()\n",
        "        elif model_type == 'linear':\n",
        "            model = LinearRegression()\n",
        "        elif model_type == 'xgboost':\n",
        "            model = xgb.XGBRegressor(random_state=42)\n",
        "        elif model_type == 'lightgbm':\n",
        "            model = lgb.LGBMRegressor(random_state=42)\n",
        "        else:\n",
        "            model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'mse': mean_squared_error(y_test, y_pred),\n",
        "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "            'mae': mean_absolute_error(y_test, y_pred),\n",
        "            'r2': r2_score(y_test, y_pred)\n",
        "        }\n",
        "\n",
        "        # Feature importance\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            self.feature_importance[model_type] = dict(zip(X_train.columns, model.feature_importances_))\n",
        "\n",
        "        return model, metrics\n",
        "\n",
        "    def train_deep_learning_model(self, X_train, y_train, X_test, y_test, problem_type='classification', target_name=''):\n",
        "        \"\"\"Train deep learning model using TensorFlow/Keras\"\"\"\n",
        "        if not DEEP_LEARNING_AVAILABLE:\n",
        "            print(\"Deep learning not available. Skipping.\")\n",
        "            # Return dummy values for consistency if DL is skipped\n",
        "            return None, {}, None\n",
        "\n",
        "        # Identify and remove constant features from training and test sets\n",
        "        # This prevents StandardScaler from producing NaNs/Infs due to zero variance\n",
        "        constant_features = X_train.columns[X_train.nunique() == 1]\n",
        "        X_train_filtered = X_train.drop(columns=constant_features)\n",
        "        X_test_filtered = X_test.drop(columns=constant_features)\n",
        "\n",
        "        # Handle case where X_train_filtered becomes empty after dropping constant features\n",
        "        if X_train_filtered.empty or X_train_filtered.shape[1] == 0:\n",
        "            print(f\"Warning: No non-constant features for deep learning model for target {target_name}. Skipping DL model.\")\n",
        "            return None, {}, None # Return dummy values\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
        "        X_test_scaled = scaler.transform(X_test_filtered)\n",
        "\n",
        "        # Replace any remaining NaNs/Infs (as a safeguard, should be rare after filtering constant features)\n",
        "        X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=1e5, neginf=-1e5)\n",
        "        X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=1e5, neginf=-1e5)\n",
        "\n",
        "        # Build model\n",
        "        model = keras.Sequential([\n",
        "            layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dense(1, activation='sigmoid' if problem_type == 'classification' else 'linear')\n",
        "        ])\n",
        "\n",
        "        # Compile model\n",
        "        if problem_type == 'classification':\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', keras.metrics.AUC()]\n",
        "            )\n",
        "            epochs = 50\n",
        "        else:\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='mse',\n",
        "                metrics=['mae', keras.metrics.RootMeanSquaredError()]\n",
        "            )\n",
        "            epochs = 100\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train_scaled, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        # Ensure y_pred is finite and valid before computing metrics\n",
        "        if problem_type == 'classification':\n",
        "            # For classification, predictions should be between 0 and 1\n",
        "            y_pred = np.nan_to_num(y_pred, nan=0.5, posinf=1.0, neginf=0.0)\n",
        "            y_pred = np.clip(y_pred, 0.0, 1.0) # Clip to ensure valid probability range\n",
        "        else:\n",
        "            y_pred = np.nan_to_num(y_pred, nan=0.0, posinf=1e10, neginf=-1e10) # For regression, use large finite numbers\n",
        "\n",
        "\n",
        "        if problem_type == 'classification':\n",
        "            y_pred_class = (y_pred > 0.5).astype(int)\n",
        "            metrics = {\n",
        "                'accuracy': accuracy_score(y_test, y_pred_class),\n",
        "                'precision': precision_score(y_test, y_pred_class, zero_division=0),\n",
        "                'recall': recall_score(y_test, y_pred_class, zero_division=0),\n",
        "                'f1': f1_score(y_test, y_pred_class, zero_division=0),\n",
        "                'auc': roc_auc_score(y_test, y_pred) if len(set(y_test)) > 1 else 0\n",
        "            }\n",
        "        else:\n",
        "            metrics = {\n",
        "                'mse': mean_squared_error(y_test, y_pred),\n",
        "                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "                'mae': mean_absolute_error(y_test, y_pred),\n",
        "                'r2': r2_score(y_test, y_pred)\n",
        "            }\n",
        "\n",
        "        # Store the Keras model along with its scaler and feature columns\n",
        "        dl_model_info = {\n",
        "            'model': model,\n",
        "            'scaler': scaler,\n",
        "            'feature_cols': X_train_filtered.columns.tolist()\n",
        "        }\n",
        "\n",
        "        return dl_model_info, metrics, history\n",
        "\n",
        "    def ensemble_voting(self, X_train, y_train, X_test, y_test, problem_type='classification'):\n",
        "        \"\"\"Create ensemble of models\"\"\"\n",
        "        if problem_type == 'classification':\n",
        "            from sklearn.ensemble import VotingClassifier\n",
        "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "            xgb_model = xgb.XGBClassifier(random_state=42)\n",
        "            lgb_model = lgb.LGBMClassifier(random_state=42)\n",
        "\n",
        "            ensemble = VotingClassifier(\n",
        "                estimators=[('rf', rf), ('xgb', xgb_model), ('lgb', lgb_model)],\n",
        "                voting='soft'\n",
        "            )\n",
        "        else:\n",
        "            from sklearn.ensemble import VotingRegressor\n",
        "            rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "            xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "            lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "            ensemble = VotingRegressor(\n",
        "                estimators=[('rf', rf), ('xgb', xgb_model), ('lgb', lgb_model)]\n",
        "            )\n",
        "\n",
        "        ensemble.fit(X_train, y_train)\n",
        "        y_pred = ensemble.predict(X_test)\n",
        "\n",
        "        if problem_type == 'classification':\n",
        "            metrics = {\n",
        "                'accuracy': accuracy_score(y_test, y_pred),\n",
        "                'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "                'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "                'f1': f1_score(y_test, y_pred, zero_division=0)\n",
        "            }\n",
        "        else:\n",
        "            metrics = {\n",
        "                'mse': mean_squared_error(y_test, y_pred),\n",
        "                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "                'mae': mean_absolute_error(y_test, y_pred),\n",
        "                'r2': r2_score(y_test, y_pred)\n",
        "            }\n",
        "\n",
        "        return ensemble, metrics\n",
        "\n",
        "    def cross_validate(self, X, y, model_type='rf', cv=5):\n",
        "        \"\"\"Perform cross-validation\"\"\"\n",
        "        if model_type == 'rf':\n",
        "            model = RandomForestClassifier(random_state=42) if len(set(y)) < 10 else RandomForestRegressor(random_state=42)\n",
        "        elif model_type == 'xgboost':\n",
        "            model = xgb.XGBClassifier(random_state=42) if len(set(y)) < 10 else xgb.XGBRegressor(random_state=42)\n",
        "\n",
        "        if len(set(y)) < 10:  # Classification\n",
        "            scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "        else:  # Regression\n",
        "            scores = cross_val_score(model, X, y, cv=cv, scoring='r2')\n",
        "\n",
        "        return scores.mean(), scores.std()\n",
        "\n",
        "# ============================================\n",
        "# 3. VIRTUAL SCREENING AND PREDICTION\n",
        "# ============================================\n",
        "\n",
        "class VirtualScreener:\n",
        "    \"\"\"Virtual screening of compounds for diabetes targets\"\"\"\n",
        "\n",
        "    def __init__(self, trained_models, all_feature_names):\n",
        "        self.models = trained_models\n",
        "        self.predictions = {}\n",
        "        self.all_feature_names = all_feature_names # General list for sklearn models\n",
        "\n",
        "    def predict_diabetes_potential(self, features_df):\n",
        "        \"\"\"Predict diabetes drug potential for new compounds\"\"\"\n",
        "        predictions = {}\n",
        "\n",
        "        for model_name, model_entry in self.models.items():\n",
        "            # Check if this entry is a Keras DL model (stored as a dict with 'model', 'scaler', 'feature_cols')\n",
        "            if isinstance(model_entry, dict) and 'model' in model_entry and 'scaler' in model_entry:\n",
        "                dl_model = model_entry['model']\n",
        "                dl_scaler = model_entry['scaler']\n",
        "                dl_feature_cols = model_entry['feature_cols']\n",
        "\n",
        "                # Prepare features_df for this specific DL model\n",
        "                temp_features_for_dl = features_df.copy()\n",
        "\n",
        "                # Ensure all features exist for the DL model, fill with 0 if missing\n",
        "                missing_in_dl_input = set(dl_feature_cols) - set(temp_features_for_dl.columns)\n",
        "                for c in missing_in_dl_input:\n",
        "                    temp_features_for_dl[c] = 0\n",
        "\n",
        "                # Ensure column order and only include features the DL model was trained on\n",
        "                dl_input_df = temp_features_for_dl[dl_feature_cols]\n",
        "\n",
        "                # Scale the input features using the DL model's specific scaler\n",
        "                dl_input_scaled = dl_scaler.transform(dl_input_df)\n",
        "                dl_input_scaled = np.nan_to_num(dl_input_scaled, nan=0.0, posinf=1e5, neginf=-1e5)\n",
        "\n",
        "                # Make prediction with the Keras model\n",
        "                pred_raw = dl_model.predict(dl_input_scaled, verbose=0) # Set verbose to 0 to suppress output\n",
        "                pred_value = pred_raw.item() if pred_raw.size == 1 else pred_raw[0].item() # Extract scalar from prediction\n",
        "\n",
        "                predictions[model_name] = pred_value\n",
        "                # For classification, Keras's single output neuron with sigmoid gives probability directly\n",
        "                predictions[f'{model_name}_probability'] = pred_value\n",
        "            else: # This is a standard sklearn model (or ensemble of sklearn models)\n",
        "                model = model_entry\n",
        "\n",
        "                # For sklearn models, use the general all_feature_names list\n",
        "                temp_features_for_sklearn = features_df.copy()\n",
        "                missing_cols = set(self.all_feature_names) - set(temp_features_for_sklearn.columns)\n",
        "                for c in missing_cols:\n",
        "                    temp_features_for_sklearn[c] = 0\n",
        "                temp_features_for_sklearn = temp_features_for_sklearn[self.all_feature_names]\n",
        "\n",
        "                pred = model.predict(temp_features_for_sklearn)\n",
        "                predictions[model_name] = pred.item() if isinstance(pred, np.ndarray) and pred.size == 1 else pred\n",
        "\n",
        "                if hasattr(model, 'predict_proba'):\n",
        "                    # Check if predict_proba is available and model is for classification\n",
        "                    if len(set(model.classes_)) > 1: # For classification models\n",
        "                        pred_proba = model.predict_proba(temp_features_for_sklearn)\n",
        "                        predictions[f'{model_name}_probability'] = pred_proba[0, 1].item() if isinstance(pred_proba[0, 1], np.ndarray) and pred_proba[0, 1].size == 1 else pred_proba[0, 1]\n",
        "\n",
        "        self.predictions = predictions\n",
        "\n",
        "        # Ensemble prediction (average of base predictions, excluding probabilities for averaging)\n",
        "        if len(predictions) > 0:\n",
        "            scalar_base_preds = []\n",
        "            for name, p in predictions.items():\n",
        "                if '_probability' not in name: # Exclude probability predictions from ensemble average if they exist as separate entries\n",
        "                    if isinstance(p, np.ndarray):\n",
        "                        scalar_base_preds.append(p.item() if p.size == 1 else p[0])\n",
        "                    elif isinstance(p, (int, float)):\n",
        "                        scalar_base_preds.append(p)\n",
        "\n",
        "            if scalar_base_preds:\n",
        "                avg_pred = np.mean(scalar_base_preds)\n",
        "                predictions['ensemble_average'] = avg_pred\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def screen_smiles(self, smiles_list):\n",
        "        \"\"\"Screen a list of SMILES strings\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for smiles in smiles_list:\n",
        "            # Calculate molecular features\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol:\n",
        "                features = self.calculate_molecular_features(mol)\n",
        "                # Predict using trained models\n",
        "                # Create a DataFrame with molecular features\n",
        "                mol_features_df = pd.DataFrame([features])\n",
        "                pred = self.predict_diabetes_potential(mol_features_df)\n",
        "                results.append({\n",
        "                    'smiles': smiles,\n",
        "                    'predictions': pred,\n",
        "                    'drug_likeness': self.calculate_drug_likeness(mol)\n",
        "                })\n",
        "            else:\n",
        "                print(f\"RDKit SMILES Parse Error for: {smiles}\") # Indicate invalid SMILES\n",
        "                results.append({\n",
        "                    'smiles': smiles,\n",
        "                    'predictions': {}, # Empty predictions for invalid SMILES\n",
        "                    'drug_likeness': {'drug_score': np.nan, 'is_druglike': False}\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def calculate_molecular_features(self, mol):\n",
        "        \"\"\"Calculate molecular features for prediction\"\"\"\n",
        "        features = {\n",
        "            'mol_weight': Descriptors.MolWt(mol),\n",
        "            'logp': Descriptors.MolLogP(mol),\n",
        "            'hbd': Descriptors.NumHDonors(mol),\n",
        "            'hba': Descriptors.NumHAcceptors(mol),\n",
        "            'tpsa': Descriptors.TPSA(mol),\n",
        "            'rotatable_bonds': Descriptors.NumRotatableBonds(mol),\n",
        "            'heavy_atoms': mol.GetNumHeavyAtoms(),\n",
        "            'ring_count': Descriptors.RingCount(mol),\n",
        "            'fraction_sp3': Descriptors.FractionCSP3(mol),\n",
        "            'num_aromatic_rings': Descriptors.NumAromaticRings(mol),\n",
        "            'num_saturated_rings': Descriptors.NumSaturatedRings(mol),\n",
        "            'num_heteroatoms': Descriptors.NumHeteroatoms(mol),\n",
        "            'mol_refractivity': Descriptors.MolMR(mol),\n",
        "            'balaban_j': Descriptors.BalabanJ(mol) if mol.GetNumAtoms() > 1 else 0,\n",
        "            'chi0v': Descriptors.Chi0v(mol),\n",
        "            'chi1v': Descriptors.Chi1v(mol),\n",
        "            'kappa1': Descriptors.Kappa1(mol),\n",
        "            'kappa2': Descriptors.Kappa2(mol),\n",
        "            'kappa3': Descriptors.Kappa3(mol),\n",
        "            'lipinski_violations': sum([\n",
        "                1 if Descriptors.MolWt(mol) > 500 else 0,\n",
        "                1 if Descriptors.MolLogP(mol) > 5 else 0,\n",
        "                1 if Descriptors.NumHDonors(mol) > 5 else 0,\n",
        "                1 if Descriptors.NumHAcceptors(mol) > 10 else 0\n",
        "            ])\n",
        "        }\n",
        "        return features\n",
        "\n",
        "    def calculate_drug_likeness(self, mol):\n",
        "        \"\"\"Calculate drug-likeness score\"\"\"\n",
        "        # QED (Quantitative Estimate of Drug-likeness)\n",
        "        try:\n",
        "            qed = Descriptors.qed(mol)\n",
        "        except:\n",
        "            qed = 0.5\n",
        "\n",
        "        # Lipinski compliance\n",
        "        lipinski_ok = sum([\n",
        "            1 if Descriptors.MolWt(mol) <= 500 else 0,\n",
        "            1 if Descriptors.MolLogP(mol) <= 5 else 0,\n",
        "            1 if Descriptors.NumHDonors(mol) <= 5 else 0,\n",
        "            1 if Descriptors.NumHAcceptors(mol) <= 10 else 0\n",
        "        ]) / 4\n",
        "\n",
        "        # Combined score\n",
        "        drug_score = (qed * 0.6 + lipinski_ok * 0.4)\n",
        "\n",
        "        return {\n",
        "            'qed': qed,\n",
        "            'lipinski_compliance': lipinski_ok,\n",
        "            'drug_score': drug_score,\n",
        "            'is_druglike': drug_score > 0.5\n",
        "        }\n",
        "\n",
        "# ============================================\n",
        "# 4. VISUALIZATION AND ANALYSIS\n",
        "# ============================================\n",
        "\n",
        "class DrugDiscoveryVisualizer:\n",
        "    \"\"\"Visualization tools for drug discovery\"\"\"\n",
        "\n",
        "    def __init__(self, trained_models=None):\n",
        "        self.figures = {}\n",
        "        self.models = trained_models # Add trained_models attribute\n",
        "\n",
        "    def plot_model_performance(self, results_dict, title=\"Model Performance\"):\n",
        "        \"\"\"Plot comparison of model performance\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        # Extract data for plotting\n",
        "        models = list(results_dict.keys())\n",
        "\n",
        "        # Classification metrics\n",
        "        # Check if 'accuracy' is in the metrics of the first model. This assumes all models have similar metrics structure\n",
        "        if models and 'accuracy' in next(iter(results_dict.values())):\n",
        "            metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1'] # Exclude roc_auc for bar plot\n",
        "            metric_data = {metric: [] for metric in metrics_to_plot}\n",
        "\n",
        "            for model_name, metrics_dict in results_dict.items():\n",
        "                for metric in metrics_to_plot:\n",
        "                    metric_data[metric].append(metrics_dict.get(metric, 0)) # Use .get for robustness\n",
        "\n",
        "            for idx, metric in enumerate(metrics_to_plot):\n",
        "                ax = axes[idx // 2, idx % 2]\n",
        "                bars = ax.bar(models, metric_data[metric])\n",
        "                ax.set_title(f'{metric.title()} Comparison')\n",
        "                ax.set_ylabel(metric.title())\n",
        "                ax.set_ylim(0, 1)\n",
        "                ax.set_xticklabels(models, rotation=45, ha='right') # Rotate and align for better readability\n",
        "\n",
        "                # Add value labels\n",
        "                for bar in bars:\n",
        "                    height = bar.get_height()\n",
        "                    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                           f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "            # Handle AUC if present, plotting it separately or in another subplot if space allows\n",
        "            # For simplicity, we can plot AUC in the remaining subplot or ignore if not explicitly asked.\n",
        "            # If roc_auc exists, it can be added to the metrics_to_plot or handled dynamically.\n",
        "            if 'roc_auc' in next(iter(results_dict.values())):\n",
        "                auc_data = [metrics_dict.get('roc_auc', 0) for metrics_dict in results_dict.values()]\n",
        "                ax_auc = axes[1,1] # Using the last subplot for AUC\n",
        "                bars_auc = ax_auc.bar(models, auc_data)\n",
        "                ax_auc.set_title('ROC AUC Comparison')\n",
        "                ax_auc.set_ylabel('ROC AUC')\n",
        "                ax_auc.set_ylim(0, 1)\n",
        "                ax_auc.set_xticklabels(models, rotation=45, ha='right')\n",
        "                for bar in bars_auc:\n",
        "                    height = bar.get_height()\n",
        "                    ax_auc.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                                f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # Regression metrics\n",
        "        elif models and 'r2' in next(iter(results_dict.values())):\n",
        "            metrics_to_plot = ['r2', 'rmse', 'mae']\n",
        "            metric_data = {metric: [] for metric in metrics_to_plot}\n",
        "\n",
        "            for model_name, metrics_dict in results_dict.items():\n",
        "                for metric in metrics_to_plot:\n",
        "                    metric_data[metric].append(metrics_dict.get(metric, 0)) # Use .get for robustness\n",
        "\n",
        "            for idx, metric in enumerate(metrics_to_plot):\n",
        "                ax = axes[idx // 2, idx % 2] # Use 0,0 0,1 for first two, then 1,0 or adjust for 3 metrics\n",
        "                bars = ax.bar(models, metric_data[metric])\n",
        "                ax.set_title(f'{metric.upper()} Comparison')\n",
        "                ax.set_ylabel(metric.upper())\n",
        "                ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "\n",
        "                # Add value labels\n",
        "                for bar in bars:\n",
        "                    height = bar.get_height()\n",
        "                    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                           f'{height:.3f}', ha='center', va='bottom')\n",
        "            # Hide unused subplot for regression if only 3 metrics\n",
        "            if len(metrics_to_plot) < 4:\n",
        "                axes[1,1].axis('off')\n",
        "        else:\n",
        "            # Handle case where results_dict is empty or metrics are unexpected\n",
        "            fig.suptitle(\"No metrics to plot or unexpected metric format\", fontsize=16)\n",
        "            for ax_row in axes:\n",
        "                for ax in ax_row:\n",
        "                    ax.axis('off')\n",
        "\n",
        "        plt.suptitle(title, fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout for suptitle\n",
        "        plt.savefig('model_performance.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        self.figures['model_performance'] = fig\n",
        "\n",
        "    def plot_feature_importance(self, feature_importance_dict, top_n=20):\n",
        "        \"\"\"Plot feature importance from models\"\"\"\n",
        "        # Filter out deep learning models as they don't have feature_importances_\n",
        "        filtered_importance_dict = {k: v for k, v in feature_importance_dict.items() if not isinstance(self.models.get(k), dict) or 'model' not in self.models.get(k)}\n",
        "\n",
        "        if not filtered_importance_dict:\n",
        "            print(\"No feature importances to plot (only DL models or empty dict).\")\n",
        "            return\n",
        "\n",
        "        num_models = len(filtered_importance_dict)\n",
        "        fig, axes = plt.subplots(num_models, 1,\n",
        "                                figsize=(12, 4 * num_models + 2)) # Adjusted figure size dynamically\n",
        "\n",
        "        if num_models == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        # Ensure axes is iterable even for 0 models\n",
        "        if not isinstance(axes, (list, np.ndarray)):\n",
        "            axes = [axes]\n",
        "\n",
        "        for idx, (model_name, importance) in enumerate(filtered_importance_dict.items()):\n",
        "            # Sort features by importance\n",
        "            # Ensure importance is a dictionary and filter out non-numeric values\n",
        "            if isinstance(importance, dict):\n",
        "                clean_importance = {k: v for k, v in importance.items() if isinstance(v, (int, float))}\n",
        "                if not clean_importance:\n",
        "                    print(f\"Warning: No numeric feature importances for {model_name}. Skipping plot.\")\n",
        "                    continue\n",
        "                sorted_features = sorted(clean_importance.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "            else:\n",
        "                print(f\"Warning: Feature importance for {model_name} is not in expected dict format. Skipping plot.\")\n",
        "                continue\n",
        "\n",
        "            if not sorted_features:\n",
        "                print(f\"No features with non-zero importance for {model_name} within top_n={top_n}.\")\n",
        "                continue\n",
        "\n",
        "            features, importance_values = zip(*sorted_features)\n",
        "\n",
        "            ax = axes[idx]\n",
        "            ax.barh(range(len(features)), importance_values)\n",
        "            ax.set_yticks(range(len(features)))\n",
        "            ax.set_yticklabels(features)\n",
        "            ax.set_xlabel('Importance')\n",
        "            ax.set_title(f'Feature Importance - {model_name}')\n",
        "            ax.invert_yaxis()  # Most important at top\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        self.figures['feature_importance'] = fig\n",
        "\n",
        "    def plot_cluster_analysis(self, X, y_pred, title=\"Compound Clustering\"):\n",
        "        \"\"\"Visualize clustering results\"\"\"\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "        # Handle case where X might have too few samples or features for dimensionality reduction\n",
        "        if X.shape[0] < 2 or X.shape[1] < 2:\n",
        "            print(\"Not enough samples or features for PCA/t-SNE. Skipping cluster plots.\")\n",
        "            plt.suptitle(title + \" (Insufficient data for PCA/t-SNE)\", fontsize=16)\n",
        "            for ax_item in axes:\n",
        "                ax_item.axis('off')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            self.figures['cluster_analysis'] = fig\n",
        "            return\n",
        "\n",
        "        # PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        X_pca = pca.fit_transform(X)\n",
        "        scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap='viridis', alpha=0.6)\n",
        "        axes[0].set_title('PCA - 2D Projection')\n",
        "        axes[0].set_xlabel('PC1')\n",
        "        axes[0].set_ylabel('PC2')\n",
        "        plt.colorbar(scatter1, ax=axes[0])\n",
        "\n",
        "        # t-SNE\n",
        "        # Adjust perplexity based on number of samples\n",
        "        perplexity_val = min(30, max(5, X.shape[0] - 1)) # Ensure perplexity is valid\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_val, init='pca', learning_rate='auto')\n",
        "        X_tsne = tsne.fit_transform(X)\n",
        "        scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred, cmap='plasma', alpha=0.6)\n",
        "        axes[1].set_title('t-SNE - 2D Projection')\n",
        "        axes[1].set_xlabel('t-SNE 1')\n",
        "        axes[1].set_ylabel('t-SNE 2')\n",
        "        plt.colorbar(scatter2, ax=axes[1])\n",
        "\n",
        "        # Cluster distribution\n",
        "        unique, counts = np.unique(y_pred, return_counts=True)\n",
        "        axes[2].bar(unique, counts, color='skyblue', edgecolor='black')\n",
        "        axes[2].set_title('Cluster Distribution')\n",
        "        axes[2].set_xlabel('Cluster')\n",
        "        axes[2].set_ylabel('Number of Compounds')\n",
        "\n",
        "        plt.suptitle(title, fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('cluster_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        self.figures['cluster_analysis'] = fig\n",
        "\n",
        "    def plot_prediction_distribution(self, y_true, y_pred, title=\"Prediction Distribution\"):\n",
        "        \"\"\"Plot distribution of predictions vs actual values\"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "        # Convert y_pred to numpy array if it's a pandas Series for consistent operations\n",
        "        if isinstance(y_pred, pd.Series):\n",
        "            y_pred = y_pred.values\n",
        "        if isinstance(y_true, pd.Series):\n",
        "            y_true = y_true.values\n",
        "\n",
        "        # Scatter plot for regression (if target has many unique values)\n",
        "        if len(np.unique(y_true)) > 10:  # Likely regression\n",
        "            # Ensure y_true and y_pred are 1D arrays\n",
        "            y_true = y_true.flatten()\n",
        "            y_pred = y_pred.flatten()\n",
        "\n",
        "            axes[0].scatter(y_true, y_pred, alpha=0.6)\n",
        "            axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()],\n",
        "                        'r--', lw=2)\n",
        "            axes[0].set_xlabel('Actual Values')\n",
        "            axes[0].set_ylabel('Predicted Values')\n",
        "            axes[0].set_title('Actual vs Predicted')\n",
        "\n",
        "            # Residuals\n",
        "            residuals = y_true - y_pred\n",
        "            axes[1].hist(residuals, bins=30, alpha=0.7, color='salmon', edgecolor='black')\n",
        "            axes[1].axvline(x=0, color='r', linestyle='--')\n",
        "            axes[1].set_xlabel('Residuals')\n",
        "            axes[1].set_ylabel('Frequency')\n",
        "            axes[1].set_title('Residual Distribution')\n",
        "\n",
        "        # Confusion matrix for classification\n",
        "        else:\n",
        "            # Ensure y_true and y_pred are integer types for confusion matrix\n",
        "            y_true = y_true.astype(int)\n",
        "            y_pred = np.round(y_pred).astype(int) # Round predictions for binary classes\n",
        "\n",
        "            # Handle potential case where y_pred contains more than 2 classes if it's a probability\n",
        "            if len(np.unique(y_pred)) > 2: # Should not happen if y_pred is correctly thresholded\n",
        "                print(\"Warning: Classification predictions have more than 2 unique values. Plotting may be unexpected.\")\n",
        "\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "            axes[0].set_xlabel('Predicted')\n",
        "            axes[0].set_ylabel('Actual')\n",
        "            axes[0].set_title('Confusion Matrix')\n",
        "\n",
        "            # Class distribution\n",
        "            unique_classes = np.unique(y_true)\n",
        "            if len(unique_classes) == 2: # Binary classification\n",
        "                class_labels = ['Negative', 'Positive']\n",
        "                class_counts_true = [np.sum(y_true == 0), np.sum(y_true == 1)]\n",
        "            else: # Multi-class classification, or more general\n",
        "                class_labels = [str(c) for c in unique_classes]\n",
        "                class_counts_true = [np.sum(y_true == c) for c in unique_classes]\n",
        "\n",
        "            axes[1].bar(class_labels, class_counts_true,\n",
        "                       color=['blue', 'red'], alpha=0.7, edgecolor='black')\n",
        "            axes[1].set_xlabel('Class')\n",
        "            axes[1].set_ylabel('Count')\n",
        "            axes[1].set_title('Class Distribution (Actual)')\n",
        "\n",
        "        plt.suptitle(title, fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('prediction_distribution.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        self.figures['prediction_distribution'] = fig\n",
        "\n",
        "# ============================================\n",
        "# 5. NOVEL DRUG CANDIDATE GENERATION WITH ML\n",
        "# ============================================\n",
        "\n",
        "class MLDrivenDrugCandidate:\n",
        "    \"\"\"Generate novel drug candidates using ML predictions\"\"\"\n",
        "\n",
        "    def __init__(self, trained_models, feature_names):\n",
        "        self.models = trained_models\n",
        "        self.feature_names = feature_names\n",
        "        self.candidates = []\n",
        "\n",
        "    def generate_candidate_from_smiles(self, smiles, candidate_name=\"ML_Candidate\"):\n",
        "        \"\"\"Generate drug candidate from SMILES using ML predictions\"\"\"\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if not mol:\n",
        "            print(f\"Invalid SMILES: {smiles}\")\n",
        "            return None\n",
        "\n",
        "        # Calculate features\n",
        "        features = self._calculate_all_features(mol)\n",
        "\n",
        "        # Make predictions using all models\n",
        "        predictions = {}\n",
        "        for model_name, model_entry in self.models.items():\n",
        "            # Check if this entry is a Keras DL model (stored as a dict)\n",
        "            if isinstance(model_entry, dict) and 'model' in model_entry and 'scaler' in model_entry:\n",
        "                dl_model = model_entry['model']\n",
        "                dl_scaler = model_entry['scaler']\n",
        "                dl_feature_cols = model_entry['feature_cols']\n",
        "\n",
        "                # Prepare features for DL model\n",
        "                temp_features_for_dl = pd.DataFrame([features])\n",
        "\n",
        "                # Ensure all features exist for the DL model, fill with 0 if missing\n",
        "                missing_in_dl_input = set(dl_feature_cols) - set(temp_features_for_dl.columns)\n",
        "                for c in missing_in_dl_input:\n",
        "                    temp_features_for_dl[c] = 0\n",
        "\n",
        "                # Ensure column order and only include features the DL model was trained on\n",
        "                dl_input_df = temp_features_for_dl[dl_feature_cols]\n",
        "\n",
        "                # Scale the input features using the DL model's specific scaler\n",
        "                dl_input_scaled = dl_scaler.transform(dl_input_df)\n",
        "                dl_input_scaled = np.nan_to_num(dl_input_scaled, nan=0.0, posinf=1e5, neginf=-1e5)\n",
        "\n",
        "                # Make prediction with the Keras model\n",
        "                pred_raw = dl_model.predict(dl_input_scaled, verbose=0)\n",
        "                pred_value = pred_raw.item() if pred_raw.size == 1 else pred_raw[0].item() # Extract scalar from prediction\n",
        "\n",
        "                predictions[model_name] = pred_value\n",
        "                predictions[f'{model_name}_probability'] = pred_value # For classification outputs\n",
        "            else: # This is a standard sklearn model (or ensemble of sklearn models)\n",
        "                model = model_entry\n",
        "\n",
        "                # Prepare features in correct format for sklearn models\n",
        "                X_single = pd.DataFrame([features])\n",
        "                # Ensure the input features_df has all columns the model was trained on\n",
        "                missing_cols = set(self.feature_names) - set(X_single.columns)\n",
        "                for c in missing_cols:\n",
        "                    X_single[c] = 0  # Fill missing (categorical) features with 0\n",
        "                # Ensure the order of columns matches the training order\n",
        "                X_single = X_single[self.feature_names]\n",
        "\n",
        "                pred = model.predict(X_single)\n",
        "                predictions[model_name] = pred.item() if isinstance(pred, np.ndarray) and pred.size == 1 else pred\n",
        "\n",
        "                if hasattr(model, 'predict_proba'):\n",
        "                    # Check if predict_proba is available and model is for classification\n",
        "                    if hasattr(model, 'classes_') and len(set(model.classes_)) > 1: # For classification models\n",
        "                        pred_proba = model.predict_proba(X_single)\n",
        "                        predictions[f'{model_name}_probability'] = pred_proba[0, 1].item() if isinstance(pred_proba[0, 1], np.ndarray) and pred_proba[0, 1].size == 1 else pred_proba[0, 1]\n",
        "\n",
        "        # Create candidate profile\n",
        "        candidate = {\n",
        "            'name': candidate_name,\n",
        "            'smiles': smiles,\n",
        "            'molecular_weight': Descriptors.MolWt(mol),\n",
        "            'logp': Descriptors.MolLogP(mol),\n",
        "            'predictions': predictions,\n",
        "            'drug_likeness': self._calculate_drug_likeness(mol),\n",
        "            'diabetes_potential': np.mean(list(predictions.values())) if predictions else 0 # Simple average, refine if needed\n",
        "        }\n",
        "\n",
        "        self.candidates.append(candidate)\n",
        "        return candidate\n",
        "\n",
        "    def generate_optimized_candidate(self, base_smiles, iterations=10):\n",
        "        \"\"\"Generate optimized candidate by modifying structure\"\"\"\n",
        "        print(f\"Optimizing candidate from base SMILES: {base_smiles}\")\n",
        "\n",
        "        best_candidate = None\n",
        "        best_score = -np.inf\n",
        "\n",
        "        for i in range(iterations):\n",
        "            # Generate modified SMILES (simplified - in practice use more sophisticated methods)\n",
        "            modified_smiles = self._mutate_smiles(base_smiles)\n",
        "            candidate = self.generate_candidate_from_smiles(\n",
        "                modified_smiles,\n",
        "                f\"Optimized_Candidate_{i+1}\"\n",
        "            )\n",
        "\n",
        "            if candidate:\n",
        "                # Consider a weighted score: 70% diabetes potential, 30% drug-likeness\n",
        "                score = candidate['diabetes_potential'] * 0.7 + candidate['drug_likeness']['drug_score'] * 0.3\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_candidate = candidate\n",
        "\n",
        "        return best_candidate\n",
        "\n",
        "    def _calculate_all_features(self, mol):\n",
        "        \"\"\"Calculate all features needed for ML models\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Basic descriptors\n",
        "        basic_features = {\n",
        "            'mol_weight': Descriptors.MolWt(mol),\n",
        "            'logp': Descriptors.MolLogP(mol),\n",
        "            'hbd': Descriptors.NumHDonors(mol),\n",
        "            'hba': Descriptors.NumHAcceptors(mol),\n",
        "            'tpsa': Descriptors.TPSA(mol),\n",
        "            'rotatable_bonds': Descriptors.NumRotatableBonds(mol),\n",
        "            'heavy_atoms': mol.GetNumHeavyAtoms(),\n",
        "            'ring_count': Descriptors.RingCount(mol),\n",
        "            'fraction_sp3': Descriptors.FractionCSP3(mol),\n",
        "            'num_aromatic_rings': Descriptors.NumAromaticRings(mol),\n",
        "            'num_saturated_rings': Descriptors.NumSaturatedRings(mol),\n",
        "            'num_heteroatoms': Descriptors.NumHeteroatoms(mol),\n",
        "            'mol_refractivity': Descriptors.MolMR(mol),\n",
        "            'balaban_j': Descriptors.BalabanJ(mol) if mol.GetNumAtoms() > 1 else 0,\n",
        "            'chi0v': Descriptors.Chi0v(mol),\n",
        "            'chi1v': Descriptors.Chi1v(mol),\n",
        "            'kappa1': Descriptors.Kappa1(mol),\n",
        "            'kappa2': Descriptors.Kappa2(mol),\n",
        "            'kappa3': Descriptors.Kappa3(mol),\n",
        "            'lipinski_violations': sum([\n",
        "                1 if Descriptors.MolWt(mol) > 500 else 0,\n",
        "                1 if Descriptors.MolLogP(mol) > 5 else 0,\n",
        "                1 if Descriptors.NumHDonors(mol) > 5 else 0,\n",
        "                1 if Descriptors.NumHAcceptors(mol) > 10 else 0\n",
        "            ])\n",
        "        }\n",
        "\n",
        "        features.update(basic_features)\n",
        "\n",
        "        # No need to add categorical features here, they are handled in generate_candidate_from_smiles\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _calculate_drug_likeness(self, mol):\n",
        "        \"\"\"Calculate comprehensive drug-likeness score\"\"\"\n",
        "        try:\n",
        "            qed = Descriptors.qed(mol)\n",
        "        except:\n",
        "            qed = 0.5\n",
        "\n",
        "        # Lipinski's Rule of Five\n",
        "        lipinski_score = sum([\n",
        "            1 if Descriptors.MolWt(mol) <= 500 else 0,\n",
        "            1 if Descriptors.MolLogP(mol) <= 5 else 0,\n",
        "            1 if Descriptors.NumHDonors(mol) <= 5 else 0,\n",
        "            1 if Descriptors.NumHAcceptors(mol) > 10 else 0\n",
        "        ]) / 4 # Adjusted Lipinski's violation logic for the score.\n",
        "\n",
        "        # Veber's rules (good oral bioavailability)\n",
        "        rotatable_bonds = Descriptors.NumRotatableBonds(mol)\n",
        "        tpsa = Descriptors.TPSA(mol)\n",
        "        veber_score = 1 if (rotatable_bonds <= 10 and tpsa <= 140) else 0.5\n",
        "\n",
        "        # Combined score\n",
        "        drug_score = (qed * 0.4 + lipinski_score * 0.3 + veber_score * 0.3)\n",
        "\n",
        "        return {\n",
        "            'qed': qed,\n",
        "            'lipinski_compliance': lipinski_score,\n",
        "            'veber_compliance': veber_score,\n",
        "            'drug_score': drug_score,\n",
        "            'is_druglike': drug_score > 0.6\n",
        "        }\n",
        "\n",
        "    def _mutate_smiles(self, smiles):\n",
        "        \"\"\"Simple SMILES mutation (for demonstration)\"\"\"\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if not mol:\n",
        "            return smiles\n",
        "\n",
        "        # Add a methyl group or change a bond (simplified)\n",
        "        # This is a very basic mutation; real-world generative models are more complex\n",
        "        from rdkit.Chem import AllChem\n",
        "        import random\n",
        "\n",
        "        # Example: add a methyl group to a random atom if possible\n",
        "        new_mol = Chem.Mol(mol) # Create a copy\n",
        "        edit_mol = Chem.EditableMol(new_mol)\n",
        "\n",
        "        atoms = [a.GetIdx() for a in new_mol.GetAtoms() if a.GetAtomicNum() != 1] # Avoid H\n",
        "        if atoms and random.random() > 0.5:\n",
        "            target_atom_idx = random.choice(atoms)\n",
        "            new_atom_idx = edit_mol.AddAtom(Chem.Atom(6)) # Add a Carbon atom\n",
        "            edit_mol.AddBond(target_atom_idx, new_atom_idx, Chem.BondType.SINGLE) # Add a single bond\n",
        "            try:\n",
        "                mutated_mol = edit_mol.GetMol()\n",
        "                Chem.SanitizeMol(mutated_mol) # Ensure valency is correct\n",
        "                return Chem.MolToSmiles(mutated_mol)\n",
        "            except Chem.AllChem.KekulizeException:\n",
        "                return smiles # Return original if sanitization fails\n",
        "            except ValueError: # Catch other potential RDKit errors during bond formation\n",
        "                return smiles\n",
        "\n",
        "        return smiles # If no mutation or failed, return original\n",
        "\n",
        "# ============================================\n",
        "# 6. MAIN PIPELINE WITH MACHINE LEARNING\n",
        "# ============================================\n",
        "\n",
        "def main_ml_pipeline():\n",
        "    \"\"\"Main pipeline with machine learning integration\"\"\"\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"MACHINE LEARNING DRIVEN DRUG DISCOVERY FOR DIABETES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # File path\n",
        "    filepath = \"MedicineOne.csv\"\n",
        "\n",
        "    # Step 1: Data Processing\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 1: DATA PROCESSING\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    processor = DrugDataProcessor(filepath)\n",
        "    processor.df = processor.load_and_clean()\n",
        "\n",
        "    # Extract features\n",
        "    processor.extract_numerical_features()\n",
        "    processor.extract_molecular_features() # This adds mol features to processor.df\n",
        "    processor.encode_categorical_features() # This adds categorical features to processor.df\n",
        "    processor.create_target_variables()\n",
        "\n",
        "    # Prepare ML data\n",
        "    ml_data = processor.prepare_ml_data()\n",
        "\n",
        "    print(f\"\\nProcessed dataset shape: {processor.df.shape}\")\n",
        "    print(f\"Numerical features: {len(processor.numerical_features)}\")\n",
        "    print(f\"Categorical features: {len(processor.categorical_features)}\")\n",
        "    print(f\"Target variables: {processor.target_columns}\")\n",
        "\n",
        "    # Step 2: Machine Learning Modeling\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 2: MACHINE LEARNING MODELING\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    ml_engine = DrugDiscoveryML()\n",
        "    trained_models = {}\n",
        "    ml_results = {}\n",
        "\n",
        "    # Train models for each target\n",
        "    for target_name, data in ml_data.items():\n",
        "        print(f\"\\nTraining models for: {target_name}\")\n",
        "\n",
        "        X = data['X']\n",
        "        y = data['y']\n",
        "\n",
        "        # Skip training if not enough samples\n",
        "        if X.shape[0] < 2:\n",
        "            print(f\"Not enough samples to train for target {target_name}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) < 10 and len(np.unique(y)) > 1 else None\n",
        "        ) # Added stratify for classification tasks\n",
        "\n",
        "        # Determine problem type\n",
        "        problem_type = 'classification' if len(set(y)) < 10 else 'regression'\n",
        "        print(f\"Problem type: {problem_type}\")\n",
        "\n",
        "        # Train multiple models\n",
        "        model_results = {}\n",
        "\n",
        "        if problem_type == 'classification':\n",
        "            # Random Forest\n",
        "            rf_model, rf_metrics = ml_engine.train_classification_model(\n",
        "                X_train, y_train, X_test, y_test, 'rf'\n",
        "            )\n",
        "            trained_models[f'{target_name}_rf'] = rf_model\n",
        "            model_results['Random Forest'] = rf_metrics\n",
        "\n",
        "            # XGBoost\n",
        "            xgb_model, xgb_metrics = ml_engine.train_classification_model(\n",
        "                X_train, y_train, X_test, y_test, 'xgboost'\n",
        "            )\n",
        "            trained_models[f'{target_name}_xgb'] = xgb_model\n",
        "            model_results['XGBoost'] = xgb_metrics\n",
        "\n",
        "            # LightGBM\n",
        "            lgb_model, lgb_metrics = ml_engine.train_classification_model(\n",
        "                X_train, y_train, X_test, y_test, 'lightgbm'\n",
        "            )\n",
        "            trained_models[f'{target_name}_lgb'] = lgb_model\n",
        "            model_results['LightGBM'] = lgb_metrics\n",
        "\n",
        "            # Ensemble\n",
        "            ensemble_model, ensemble_metrics = ml_engine.ensemble_voting(\n",
        "                X_train, y_train, X_test, y_test, 'classification'\n",
        "            )\n",
        "            trained_models[f'{target_name}_ensemble'] = ensemble_model\n",
        "            model_results['Ensemble'] = ensemble_metrics\n",
        "\n",
        "        else:  # Regression\n",
        "            # Random Forest\n",
        "            rf_model, rf_metrics = ml_engine.train_regression_model(\n",
        "                X_train, y_train, X_test, y_test, 'rf'\n",
        "            )\n",
        "            trained_models[f'{target_name}_rf'] = rf_model\n",
        "            model_results['Random Forest'] = rf_metrics\n",
        "\n",
        "            # XGBoost\n",
        "            xgb_model, xgb_metrics = ml_engine.train_regression_model(\n",
        "                X_train, y_train, X_test, y_test, 'xgboost'\n",
        "            )\n",
        "            trained_models[f'{target_name}_xgb'] = xgb_model\n",
        "            model_results['XGBoost'] = xgb_metrics\n",
        "\n",
        "            # LightGBM\n",
        "            lgb_model, lgb_metrics = ml_engine.train_regression_model(\n",
        "                X_train, y_train, X_test, y_test, 'lightgbm'\n",
        "            )\n",
        "            trained_models[f'{target_name}_lgb'] = lgb_model\n",
        "            model_results['LightGBM'] = lgb_metrics\n",
        "\n",
        "        # Deep Learning (if available)\n",
        "        if DEEP_LEARNING_AVAILABLE and len(X_train) > 100:\n",
        "            print(\"Training Deep Learning model...\")\n",
        "            dl_model_info, dl_metrics, dl_history = ml_engine.train_deep_learning_model(\n",
        "                X_train, y_train, X_test, y_test, problem_type, target_name\n",
        "            )\n",
        "            if dl_model_info: # dl_model_info will be a dictionary if training was successful\n",
        "                trained_models[f'{target_name}_dl'] = dl_model_info # Store the dict directly\n",
        "                model_results['Deep Learning'] = dl_metrics\n",
        "\n",
        "        # Store results\n",
        "        ml_results[target_name] = model_results\n",
        "\n",
        "    # Instantiate Visualizer AFTER trained_models is populated\n",
        "    visualizer = DrugDiscoveryVisualizer(trained_models=trained_models)\n",
        "\n",
        "    # Visualize model performance (only if models were trained)\n",
        "    for target_name, model_results in ml_results.items():\n",
        "        if model_results:\n",
        "            visualizer.plot_model_performance(model_results,\n",
        "                                            title=f\"Model Performance for {target_name}\")\n",
        "\n",
        "            # Find relevant X_test and y_test for prediction distribution\n",
        "            data = ml_data[target_name]\n",
        "            X = data['X']\n",
        "            y = data['y']\n",
        "            # Re-split to get X_test, y_test consistent with training\n",
        "            _, X_test_for_plot, _, y_test_for_plot = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) < 10 and len(np.unique(y)) > 1 else None\n",
        "            )\n",
        "\n",
        "            problem_type = 'classification' if len(set(y_test_for_plot)) < 10 else 'regression'\n",
        "\n",
        "            y_pred_for_plot = None\n",
        "            if problem_type == 'classification':\n",
        "                # Prioritize ensemble if available\n",
        "                ensemble_key = f'{target_name}_ensemble'\n",
        "                if ensemble_key in trained_models:\n",
        "                    ensemble_model = trained_models[ensemble_key]\n",
        "                    y_pred_for_plot = ensemble_model.predict(X_test_for_plot)\n",
        "                else:\n",
        "                    # Fallback to RF if no ensemble\n",
        "                    rf_key = f'{target_name}_rf'\n",
        "                    if rf_key in trained_models:\n",
        "                        rf_model = trained_models[rf_key]\n",
        "                        y_pred_for_plot = rf_model.predict(X_test_for_plot)\n",
        "\n",
        "            else: # Regression\n",
        "                rf_key = f'{target_name}_rf'\n",
        "                if rf_key in trained_models:\n",
        "                    rf_model = trained_models[rf_key]\n",
        "                    y_pred_for_plot = rf_model.predict(X_test_for_plot)\n",
        "\n",
        "            if y_pred_for_plot is not None:\n",
        "                visualizer.plot_prediction_distribution(y_test_for_plot, y_pred_for_plot,\n",
        "                                                       title=f\"Predictions for {target_name}\")\n",
        "            else:\n",
        "                print(f\"Could not generate prediction distribution plot for {target_name} due to missing model or predictions.\")\n",
        "\n",
        "\n",
        "    # Step 3: Feature Importance Analysis\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 3: FEATURE IMPORTANCE ANALYSIS\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Collect feature importances from all relevant models\n",
        "    all_feature_importance = {}\n",
        "    # We need a sample X (features) to get column names for feature importance plots.\n",
        "    # Using X from the last target processed or getting it dynamically if needed.\n",
        "    # For this fix, assume X from ml_data[target_name] is representative.\n",
        "    if ml_data:\n",
        "        sample_X_for_features = list(ml_data.values())[0]['X']\n",
        "    else:\n",
        "        sample_X_for_features = pd.DataFrame() # Empty DataFrame if no data\n",
        "\n",
        "    for model_key, model_obj in trained_models.items():\n",
        "        model = model_obj['model'] if isinstance(model_obj, dict) else model_obj # Handle DL model structure\n",
        "        # Ensure model is not None and has feature importance or coefficients\n",
        "        if model:\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                if not sample_X_for_features.empty:\n",
        "                    all_feature_importance[model_key] = dict(zip(sample_X_for_features.columns, model.feature_importances_))\n",
        "                else:\n",
        "                    print(f\"Warning: Cannot get feature names for {model_key} as sample_X_for_features is empty.\")\n",
        "            elif hasattr(model, 'coef_'):\n",
        "                # This path is usually for linear models; ensure it's not a DL model dict\n",
        "                if not isinstance(model_obj, dict) and not sample_X_for_features.empty:\n",
        "                    # For logistic regression or linear regression, coef_ is typically 1D or 2D (multi-class)\n",
        "                    if model.coef_.ndim == 1:\n",
        "                        all_feature_importance[model_key] = dict(zip(sample_X_for_features.columns, np.abs(model.coef_)))\n",
        "                    elif model.coef_.ndim == 2 and model.coef_.shape[0] > 0: # Multi-class case\n",
        "                        # Take average of absolute coefficients or first class's coefficients\n",
        "                        all_feature_importance[model_key] = dict(zip(sample_X_for_features.columns, np.abs(model.coef_[0])))\n",
        "                elif sample_X_for_features.empty:\n",
        "                    print(f\"Warning: Cannot get feature names for {model_key} as sample_X_for_features is empty.\")\n",
        "\n",
        "    if all_feature_importance:\n",
        "        visualizer.plot_feature_importance(all_feature_importance)\n",
        "    else:\n",
        "        print(\"No feature importances to plot.\")\n",
        "\n",
        "\n",
        "    # Step 4: Clustering Analysis\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 4: CLUSTERING ANALYSIS\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Use features for clustering\n",
        "    features_for_clustering = [f for f in processor.numerical_features + processor.categorical_features if f in processor.df.columns]\n",
        "    if len(features_for_clustering) == 0:\n",
        "        print(\"No features available for clustering. Skipping.\")\n",
        "        cluster_labels = np.array([])\n",
        "    else:\n",
        "        X_cluster = processor.df[features_for_clustering].fillna(0)\n",
        "\n",
        "        # Handle clustering only if enough samples\n",
        "        if X_cluster.shape[0] > 1 and X_cluster.shape[1] > 0:\n",
        "            kmeans = KMeans(n_clusters=min(5, X_cluster.shape[0] -1), random_state=42, n_init=10) # Ensure n_clusters is valid\n",
        "            cluster_labels = kmeans.fit_predict(X_cluster)\n",
        "\n",
        "            # Add cluster labels to dataframe\n",
        "            processor.df['cluster'] = cluster_labels\n",
        "\n",
        "            # Visualize clusters\n",
        "            visualizer.plot_cluster_analysis(X_cluster, cluster_labels,\n",
        "                                            title=\"Drug Compound Clustering\")\n",
        "\n",
        "            # Analyze clusters\n",
        "            print(\"\\nCluster Analysis:\")\n",
        "            for cluster_id in range(kmeans.n_clusters):\n",
        "                cluster_drugs = processor.df[processor.df['cluster'] == cluster_id]\n",
        "                print(f\"\\nCluster {cluster_id}: {len(cluster_drugs)} drugs\")\n",
        "                if len(cluster_drugs) > 0:\n",
        "                    avg_efficacy = cluster_drugs['Efficacy %_numeric'].mean() if 'Efficacy %_numeric' in cluster_drugs.columns else np.nan\n",
        "                    # The requested change from 'Bioavailability %_numeric' to 'Bioavailability_numeric' is already present.\n",
        "                    avg_bioavailability = cluster_drugs['Bioavailability_numeric'].mean() if 'Bioavailability_numeric' in cluster_drugs.columns else np.nan\n",
        "                    diabetes_drugs = cluster_drugs['is_diabetes_drug'].sum() if 'is_diabetes_drug' in cluster_drugs.columns else 0\n",
        "                    print(f\"  Avg Efficacy: {avg_efficacy:.1f}%\")\n",
        "                    print(f\"  Avg Bioavailability: {avg_bioavailability:.1f}%\")\n",
        "                    print(f\"  Diabetes drugs: {diabetes_drugs}\")\n",
        "        else:\n",
        "            print(\"Not enough data for clustering analysis. Skipping.\")\n",
        "            processor.df['cluster'] = -1 # Assign a default/no-cluster label\n",
        "            cluster_labels = np.array([]) # Empty labels\n",
        "\n",
        "\n",
        "    # Step 5: Virtual Screening\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 5: VIRTUAL SCREENING\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Pass all feature names to VirtualScreener\n",
        "    all_ml_feature_names = processor.numerical_features + processor.categorical_features\n",
        "    screener = VirtualScreener(trained_models, all_ml_feature_names)\n",
        "\n",
        "    # Test with some hypothetical SMILES (corrected malformed Lisinopril-like SMILES to Toluene)\n",
        "    test_smiles = [\n",
        "        \"CC(C)NCC(O)CC1=CC=C(C=C1)OC2=NC=C(C=N2)C3=CC=CC=C3\",  # Our hypothetical candidate\n",
        "        \"CC1=CC=C(C=C1)C(C)(C)CC(C2=CC=C(C=C2)C(=O)O)O\",  # Similar to Atorvastatin\n",
        "        \"CN(C)C(=N)N=C(N)N\",  # Metformin-like\n",
        "        \"Cc1ccccc1\", # Toluene (simple, valid SMILES)\n",
        "    ]\n",
        "\n",
        "    screening_results = screener.screen_smiles(test_smiles)\n",
        "\n",
        "    print(\"\\nVirtual Screening Results:\")\n",
        "    for _, result in screening_results.iterrows():\n",
        "        print(f\"\\nSMILES: {result['smiles'][:50]}...\")\n",
        "        print(f\"Drug-likeness Score: {result['drug_likeness']['drug_score']:.3f}\")\n",
        "        # Access 'ensemble_average' if available, otherwise just print all predictions\n",
        "        if 'predictions' in result and 'ensemble_average' in result['predictions']:\n",
        "            diabetes_potential = result['predictions']['ensemble_average']\n",
        "            if isinstance(diabetes_potential, np.ndarray):\n",
        "                diabetes_potential = diabetes_potential.item()\n",
        "            print(f\"Diabetes Potential Score: {diabetes_potential:.3f}\")\n",
        "        else:\n",
        "            print(\"  No ensemble average prediction available. Individual model predictions:\")\n",
        "            for p_name, p_val in result['predictions'].items():\n",
        "                if '_probability' not in p_name and isinstance(p_val, (int, float, np.number)):\n",
        "                    print(f\"    {p_name}: {p_val:.3f}\")\n",
        "\n",
        "    # Step 6: Generate Novel Candidates with ML\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 6: GENERATING NOVEL CANDIDATES WITH ML\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Use diabetes prediction model for candidate generation\n",
        "    diabetes_models = {k: v for k, v in trained_models.items() if 'is_diabetes_drug' in k}\n",
        "\n",
        "    novel_candidate = None\n",
        "    optimized_candidate = None\n",
        "    if diabetes_models:\n",
        "        candidate_generator = MLDrivenDrugCandidate(\n",
        "            diabetes_models,\n",
        "            processor.numerical_features + processor.categorical_features\n",
        "        )\n",
        "\n",
        "        # Generate candidate from base SMILES\n",
        "        base_smiles = \"CC(C)NCC(O)CC1=CC=C(C=C1)OC2=NC=C(C=N2)C3=CC=CC=C3\"\n",
        "        novel_candidate = candidate_generator.generate_candidate_from_smiles(\n",
        "            base_smiles, \"ML_Diabetes_Candidate_1\"\n",
        "        )\n",
        "\n",
        "        if novel_candidate:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"ML-GENERATED NOVEL DIABETES DRUG CANDIDATE\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            print(f\"\\nCandidate Name: {novel_candidate['name']}\")\n",
        "            print(f\"SMILES: {novel_candidate['smiles']}\")\n",
        "            print(f\"Molecular Weight: {novel_candidate['molecular_weight']:.2f} Da\")\n",
        "            print(f\"LogP: {novel_candidate['logp']:.2f}\")\n",
        "            print(f\"Drug-likeness Score: {novel_candidate['drug_likeness']['drug_score']:.3f}\")\n",
        "            print(f\"Diabetes Potential Score: {novel_candidate['diabetes_potential']:.3f}\")\n",
        "\n",
        "            print(\"\\nML Model Predictions:\")\n",
        "            for model_name, prediction in novel_candidate['predictions'].items():\n",
        "                if '_probability' not in model_name and isinstance(prediction, (int, float, np.number)):\n",
        "                    print(f\"  {model_name}: {prediction:.3f}\")\n",
        "\n",
        "            print(\"\\nDrug-likeness Analysis:\")\n",
        "            for key, value in novel_candidate['drug_likeness'].items():\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"  {key}: {value:.3f}\")\n",
        "                else:\n",
        "                    print(f\"  {key}: {value}\")\n",
        "\n",
        "        # Try optimization\n",
        "        print(\"\\n\" + \"-\" * 40)\n",
        "        print(\"Attempting candidate optimization...\")\n",
        "        optimized_candidate = candidate_generator.generate_optimized_candidate(base_smiles, iterations=5)\n",
        "\n",
        "        if optimized_candidate:\n",
        "            print(f\"\\nOptimized Candidate: {optimized_candidate['name']}\")\n",
        "            print(f\"Optimized SMILES: {optimized_candidate['smiles']}\")\n",
        "            print(f\"Optimized Diabetes Potential: {optimized_candidate['diabetes_potential']:.3f}\")\n",
        "            print(f\"Optimized Drug-likeness: {optimized_candidate['drug_likeness']['drug_score']:.3f}\")\n",
        "    else:\n",
        "        print(\"No diabetes prediction models available for candidate generation.\")\n",
        "\n",
        "    # Step 7: Generate Comprehensive Report\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"STEP 7: GENERATING COMPREHENSIVE REPORT\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Save results\n",
        "    with open('ml_drug_discovery_report.txt', 'w') as f:\n",
        "        f.write(\"=\" * 70 + \"\\n\")\n",
        "        f.write(\"MACHINE LEARNING DRUG DISCOVERY REPORT - DIABETES\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"1. DATASET SUMMARY\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        f.write(f\"Total drugs in dataset: {len(processor.df)}\\n\")\n",
        "        f.write(f\"Diabetes drugs identified: {processor.df['is_diabetes_drug'].sum()}\\n\")\n",
        "        f.write(f\"High efficacy drugs (>80%): {processor.df['high_efficacy'].sum()}\\n\\n\")\n",
        "\n",
        "        f.write(\"2. MACHINE LEARNING PERFORMANCE\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        for target_name, model_results in ml_results.items():\n",
        "            f.write(f\"\\nTarget: {target_name}\\n\")\n",
        "            for model_name, metrics in model_results.items():\n",
        "                f.write(f\"  {model_name}:\\n\")\n",
        "                for metric_name, metric_value in metrics.items():\n",
        "                    if metric_name != 'confusion_matrix':\n",
        "                        f.write(f\"    {metric_name}: {metric_value:.4f}\\n\")\n",
        "\n",
        "        f.write(\"\\n3. CLUSTERING ANALYSIS\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        if cluster_labels.size > 0:\n",
        "            for cluster_id in range(kmeans.n_clusters):\n",
        "                cluster_drugs = processor.df[processor.df['cluster'] == cluster_id]\n",
        "                diabetes_count = cluster_drugs['is_diabetes_drug'].sum() if 'is_diabetes_drug' in cluster_drugs.columns else 0\n",
        "                f.write(f\"\\nCluster {cluster_id}: {len(cluster_drugs)} drugs\")\n",
        "                f.write(f\" (Diabetes drugs: {diabetes_count})\\n\")\n",
        "        else:\n",
        "            f.write(\"Clustering analysis skipped due to insufficient data.\\n\")\n",
        "\n",
        "        f.write(\"\\n4. VIRTUAL SCREENING RESULTS\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        for _, result in screening_results.iterrows():\n",
        "            f.write(f\"\\nSMILES: {result['smiles']}\\n\")\n",
        "            f.write(f\"  Drug Score: {result['drug_likeness']['drug_score']:.3f}\\n\")\n",
        "            if 'predictions' in result and 'ensemble_average' in result['predictions']:\n",
        "                diabetes_potential = result['predictions']['ensemble_average']\n",
        "                if isinstance(diabetes_potential, np.ndarray):\n",
        "                    diabetes_potential = diabetes_potential.item()\n",
        "                f.write(f\"  Diabetes Potential: {diabetes_potential:.3f}\\n\")\n",
        "            else:\n",
        "                f.write(\"  No ensemble average prediction available.\\n\")\n",
        "\n",
        "        f.write(\"\\n5. NOVEL CANDIDATE GENERATION\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        if novel_candidate:\n",
        "            f.write(f\"\\nCandidate: {novel_candidate['name']}\\n\")\n",
        "            f.write(f\"SMILES: {novel_candidate['smiles']}\\n\")\n",
        "            f.write(f\"Molecular Weight: {novel_candidate['molecular_weight']:.2f} Da\\n\")\n",
        "            f.write(f\"LogP: {novel_candidate['logp']:.2f}\\n\")\n",
        "            f.write(f\"Diabetes Potential: {novel_candidate['diabetes_potential']:.3f}\\n\")\n",
        "            f.write(f\"Drug-likeness: {novel_candidate['drug_likeness']['drug_score']:.3f}\\n\")\n",
        "        else:\n",
        "            f.write(\"No novel candidate generated.\\n\")\n",
        "\n",
        "        f.write(\"\\n6. RECOMMENDATIONS\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        f.write(\"1. Prioritize compounds with high ML-predicted diabetes potential\\n\")\n",
        "        f.write(\"2. Focus on clusters with existing diabetes drugs\\n\")\n",
        "        f.write(\"3. Optimize drug-likeness properties while maintaining efficacy\\n\")\n",
        "        f.write(\"4. Consider dual/triple mechanisms for better outcomes\\n\")\n",
        "        f.write(\"5. Validate top candidates with molecular docking studies\\n\")\n",
        "\n",
        "    # Step 8: Create Summary Table\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SUMMARY OF ML-GENERATED CANDIDATES\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    summary_data = []\n",
        "    if 'candidate_generator' in locals():\n",
        "        for candidate in candidate_generator.candidates:\n",
        "            summary_data.append({\n",
        "                'Name': candidate['name'],\n",
        "                'MW': f\"{candidate['molecular_weight']:.1f}\",\n",
        "                'LogP': f\"{candidate['logp']:.2f}\",\n",
        "                'Drug Score': f\"{candidate['drug_likeness']['drug_score']:.3f}\",\n",
        "                'Diabetes Potential': f\"{candidate['diabetes_potential']:.3f}\",\n",
        "                'QED': f\"{candidate['drug_likeness']['qed']:.3f}\",\n",
        "                'Lipinski OK': candidate['drug_likeness']['lipinski_compliance'] > 0.75\n",
        "            })\n",
        "\n",
        "    if summary_data:\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        print(summary_df.to_string(index=False))\n",
        "\n",
        "        # Save summary to CSV\n",
        "        summary_df.to_csv('ml_generated_candidates.csv', index=False)\n",
        "        print(\"\\nSummary saved to 'ml_generated_candidates.csv'\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"\\nGenerated Files:\")\n",
        "    print(\"1. model_performance.png - Model comparison plots\")\n",
        "    print(\"2. feature_importance.png - Feature importance analysis\")\n",
        "    print(\"3. cluster_analysis.png - Clustering visualization\")\n",
        "    print(\"4. prediction_distribution.png - Prediction analysis\")\n",
        "    print(\"5. ml_drug_discovery_report.txt - Comprehensive report\")\n",
        "    print(\"6. ml_generated_candidates.csv - ML-generated candidates\")\n",
        "\n",
        "    return {\n",
        "        'dataframe': processor.df,\n",
        "        'trained_models': trained_models,\n",
        "        'ml_results': ml_results,\n",
        "        'candidates': candidate_generator.candidates if 'candidate_generator' in locals() else []\n",
        "    }\n",
        "\n",
        "# ============================================\n",
        "# 7. EXECUTION\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check for required libraries\n",
        "    required_libraries = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'sklearn', 'rdkit']\n",
        "\n",
        "    missing_libs = []\n",
        "    for lib in required_libraries:\n",
        "        try:\n",
        "            __import__(lib)\n",
        "        except ImportError:\n",
        "            missing_libs.append(lib)\n",
        "\n",
        "    if missing_libs:\n",
        "        print(f\"Missing libraries: {', '.join(missing_libs)}\")\n",
        "        print(\"Please install them using:\")\n",
        "        print(\"pip install pandas numpy matplotlib seaborn scikit-learn rdkit-pypi xgboost lightgbm\")\n",
        "\n",
        "        # Try to run with available libraries\n",
        "        print(\"\\nAttempting to run with available libraries...\")\n",
        "        try:\n",
        "            results = main_ml_pipeline()\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            print(\"\\nPlease install all required libraries for full functionality.\")\n",
        "    else:\n",
        "        # Run the complete pipeline\n",
        "        results = main_ml_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e557cfc",
        "outputId": "0524ab51-148c-4a8e-d7bb-7fa7334ab30c"
      },
      "source": [
        "print('--- Confusion Matrices for Classification Models ---')\n",
        "\n",
        "for target_name, model_results in results['ml_results'].items():\n",
        "    # Check if this target was a classification task (by checking for 'confusion_matrix' in any model's metrics)\n",
        "    is_classification = False\n",
        "    for model_name, metrics in model_results.items():\n",
        "        if 'confusion_matrix' in metrics:\n",
        "            is_classification = True\n",
        "            break\n",
        "\n",
        "    if is_classification:\n",
        "        print(f\"\\nTarget: {target_name}\")\n",
        "        for model_name, metrics in model_results.items():\n",
        "            if 'confusion_matrix' in metrics:\n",
        "                print(f\"  Model: {model_name}\")\n",
        "                print(\"    Confusion Matrix:\")\n",
        "                print(metrics['confusion_matrix'])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Confusion Matrices for Classification Models ---\n",
            "\n",
            "Target: is_diabetes_drug\n",
            "  Model: Random Forest\n",
            "    Confusion Matrix:\n",
            "[[72  0]\n",
            " [ 1  9]]\n",
            "  Model: XGBoost\n",
            "    Confusion Matrix:\n",
            "[[71  1]\n",
            " [ 1  9]]\n",
            "  Model: LightGBM\n",
            "    Confusion Matrix:\n",
            "[[70  2]\n",
            " [ 1  9]]\n",
            "\n",
            "Target: high_efficacy\n",
            "  Model: Random Forest\n",
            "    Confusion Matrix:\n",
            "[[60  0]\n",
            " [ 0 22]]\n",
            "  Model: XGBoost\n",
            "    Confusion Matrix:\n",
            "[[60  0]\n",
            " [ 0 22]]\n",
            "  Model: LightGBM\n",
            "    Confusion Matrix:\n",
            "[[60  0]\n",
            " [ 0 22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'results' dictionary from the previous run is available.\n",
        "\n",
        "print(\"--- Comparing Metformin and ML_Diabetes_Candidate_1 ---\")\n",
        "\n",
        "# --- Get data for ML_Diabetes_Candidate_1 ---\n",
        "ml_candidate_info = None\n",
        "for candidate in results['candidates']:\n",
        "    if candidate['name'] == 'ML_Diabetes_Candidate_1':\n",
        "        ml_candidate_info = candidate\n",
        "        break\n",
        "\n",
        "if ml_candidate_info:\n",
        "    ml_cand_smiles = ml_candidate_info['smiles']\n",
        "    ml_cand_drug_likeness = ml_candidate_info['drug_likeness']['drug_score']\n",
        "    # Explicitly get the ensemble prediction for is_diabetes_drug classification\n",
        "    ml_cand_diabetes_potential_ensemble = ml_candidate_info['predictions'].get('is_diabetes_drug_ensemble', np.nan)\n",
        "    if isinstance(ml_cand_diabetes_potential_ensemble, np.ndarray):\n",
        "        ml_cand_diabetes_potential_ensemble = ml_cand_diabetes_potential_ensemble.item()\n",
        "\n",
        "    print(\"\\nML_Diabetes_Candidate_1 Details:\")\n",
        "    print(f\"  SMILES: {ml_cand_smiles}\")\n",
        "    print(f\"  Drug-likeness Score: {ml_cand_drug_likeness:.3f}\")\n",
        "    print(f\"  Diabetes Potential (is_diabetes_drug_ensemble): {ml_cand_diabetes_potential_ensemble:.3f}\")\n",
        "else:\n",
        "    print(\"Error: ML_Diabetes_Candidate_1 not found in the generated candidates.\")\n",
        "    ml_cand_smiles, ml_cand_drug_likeness, ml_cand_diabetes_potential_ensemble = \"N/A\", np.nan, np.nan\n",
        "\n",
        "\n",
        "# --- Get data for Metformin-like compound ---\n",
        "metformin_smiles = \"CN(C)C(=N)N=C(N)N\" # Metformin SMILES\n",
        "\n",
        "# Re-initialize the VirtualScreener with trained models and feature names\n",
        "trained_models = results['trained_models']\n",
        "# Extract feature names from one of the trained models\n",
        "# (assuming all models in trained_models were trained on the same feature set and have 'feature_names_in_')\n",
        "try:\n",
        "    all_ml_feature_names = trained_models[list(trained_models.keys())[0]].feature_names_in_.tolist()\n",
        "except AttributeError:\n",
        "    # Fallback if feature_names_in_ is not available or if the model type does not support it\n",
        "    # This part needs to be robust if the original 'processor' object is not available.\n",
        "    # For simplicity, we assume ml_data structure from results can give feature names\n",
        "    print(\"Warning: Could not get feature names directly from a trained model. Inferring from ml_data.\")\n",
        "    first_target_name = list(results['ml_results'].keys())[0]\n",
        "    X_sample = results['ml_results'][first_target_name]['X']\n",
        "    all_ml_feature_names = X_sample.columns.tolist()\n",
        "\n",
        "# Create a new screener instance\n",
        "screener = VirtualScreener(trained_models, all_ml_feature_names)\n",
        "\n",
        "# Screen Metformin SMILES\n",
        "metformin_screening_results_df = screener.screen_smiles([metformin_smiles])\n",
        "\n",
        "metformin_info = metformin_screening_results_df.iloc[0]\n",
        "metformin_drug_likeness = metformin_info['drug_likeness']['drug_score']\n",
        "metformin_diabetes_potential_ensemble = metformin_info['predictions'].get('is_diabetes_drug_ensemble', np.nan)\n",
        "if isinstance(metformin_diabetes_potential_ensemble, np.ndarray):\n",
        "    metformin_diabetes_potential_ensemble = metformin_diabetes_potential_ensemble.item()\n",
        "\n",
        "print(\"\\nMetformin Details (from Virtual Screening):\")\n",
        "print(f\"  SMILES: {metformin_smiles}\")\n",
        "print(f\"  Drug-likeness Score: {metformin_drug_likeness:.3f}\")\n",
        "print(f\"  Diabetes Potential (is_diabetes_drug_ensemble): {metformin_diabetes_potential_ensemble:.3f}\")\n",
        "\n",
        "\n",
        "# --- Final Comparison and Conclusion ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"             FINAL DRUG CANDIDATE COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "print(f\"{'Metric':<30} | {'Metformin':<15} | {'ML_Diabetes_Candidate_1':<25}\")\n",
        "print(\"-\" * 75)\n",
        "print(f\"{'Drug-likeness Score':<30} | {metformin_drug_likeness:<15.3f} | {ml_cand_drug_likeness:<25.3f}\")\n",
        "print(f\"{'Diabetes Potential (Ensemble)':<30} | {metformin_diabetes_potential_ensemble:<15.3f} | {ml_cand_diabetes_potential_ensemble:<25.3f}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "if ml_cand_diabetes_potential_ensemble > metformin_diabetes_potential_ensemble and ml_cand_drug_likeness > metformin_drug_likeness:\n",
        "    print(\"\\nConclusion: ML_Diabetes_Candidate_1 shows promising results, surpassing Metformin in both predicted diabetes potential (classification) and drug-likeness score. It represents a potentially superior candidate.\")\n",
        "elif ml_cand_diabetes_potential_ensemble > metformin_diabetes_potential_ensemble:\n",
        "    print(\"\\nConclusion: ML_Diabetes_Candidate_1 has a higher predicted diabetes potential (classification) than Metformin, but Metformin has a higher drug-likeness score. Its efficacy for diabetes is higher but there is a trade-off in overall drug-like properties compared to Metformin.\")\n",
        "elif metformin_diabetes_potential_ensemble > ml_cand_diabetes_potential_ensemble:\n",
        "    print(\"\\nConclusion: Metformin has a higher predicted diabetes potential (classification) based on the ensemble model. While ML_Diabetes_Candidate_1 has a higher drug-likeness score, its diabetes potential is lower. This suggests Metformin is a better choice for diabetes efficacy according to the model, while the ML candidate might be generally more 'drug-like' overall.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: Based on the current model predictions for diabetes potential (classification) and drug-likeness, the two compounds show mixed results. Further analysis or weighting of criteria would be needed to definitively determine which is 'best'.\")\n",
        "\n",
        "print(\"=\"*50)\n"
      ],
      "metadata": {
        "id": "fLbyDym-i-o1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a0f52ad-ea51-482a-975c-cceb0552e5de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Comparing Metformin and ML_Diabetes_Candidate_1 ---\n",
            "\n",
            "ML_Diabetes_Candidate_1 Details:\n",
            "  SMILES: CC(C)NCC(O)CC1=CC=C(C=C1)OC2=NC=C(C=N2)C3=CC=CC=C3\n",
            "  Drug-likeness Score: 0.780\n",
            "  Diabetes Potential (is_diabetes_drug_ensemble): 0.000\n",
            "\n",
            "Metformin Details (from Virtual Screening):\n",
            "  SMILES: CN(C)C(=N)N=C(N)N\n",
            "  Drug-likeness Score: 0.569\n",
            "  Diabetes Potential (is_diabetes_drug_ensemble): 0.000\n",
            "\n",
            "==================================================\n",
            "             FINAL DRUG CANDIDATE COMPARISON\n",
            "==================================================\n",
            "Metric                         | Metformin       | ML_Diabetes_Candidate_1  \n",
            "---------------------------------------------------------------------------\n",
            "Drug-likeness Score            | 0.569           | 0.780                    \n",
            "Diabetes Potential (Ensemble)  | 0.000           | 0.000                    \n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "Conclusion: Based on the current model predictions for diabetes potential (classification) and drug-likeness, the two compounds show mixed results. Further analysis or weighting of criteria would be needed to definitively determine which is 'best'.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The comparison was already run in the previous step.\n",
        "# Here are the results:\n",
        "\n",
        "print(\"\\n--- Summary of Metformin vs ML_Diabetes_Candidate_1 ---\")\n",
        "print(f\"ML_Diabetes_Candidate_1 Drug-likeness Score: {ml_cand_drug_likeness:.3f}\")\n",
        "print(f\"ML_Diabetes_Candidate_1 Diabetes Potential (Ensemble): {ml_cand_diabetes_potential_ensemble:.3f}\")\n",
        "print(f\"Metformin Drug-likeness Score: {metformin_drug_likeness:.3f}\")\n",
        "print(f\"Metformin Diabetes Potential (Ensemble): {metformin_diabetes_potential_ensemble:.3f}\")\n",
        "\n",
        "print(\"\\nConclusion: Based on these results, ML_Diabetes_Candidate_1 has a higher predicted drug-likeness score. However, both compounds have a very low predicted diabetes potential from the ensemble model. Further investigation or different models might be needed to confirm diabetes efficacy for both compounds.\")"
      ],
      "metadata": {
        "id": "1y7yi-WXjuAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6c9aa04"
      },
      "source": [
        "print('Gradient Boosting Accuracies:')\n",
        "for target, accuracies in gradient_boosting_accuracies.items():\n",
        "    print(f\"Target: {target}\")\n",
        "    for model, accuracy in accuracies.items():\n",
        "        print(f\"  {model}: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ad23def"
      },
      "source": [
        "gradient_boosting_accuracies = {}\n",
        "\n",
        "for target, models_results in results['ml_results'].items():\n",
        "    # Check if 'accuracy' metric exists for any model to identify classification tasks\n",
        "    if any('accuracy' in metrics for metrics in models_results.values()):\n",
        "        target_accuracies = {}\n",
        "        if 'XGBoost' in models_results:\n",
        "            target_accuracies['XGBoost'] = models_results['XGBoost'].get('accuracy')\n",
        "        if 'LightGBM' in models_results:\n",
        "            target_accuracies['LightGBM'] = models_results['LightGBM'].get('accuracy')\n",
        "\n",
        "        if target_accuracies:\n",
        "            gradient_boosting_accuracies[target] = target_accuracies\n",
        "\n",
        "print(\"Extracted Gradient Boosting Accuracies:\")\n",
        "for target, accuracies in gradient_boosting_accuracies.items():\n",
        "    print(f\"Target: {target}\")\n",
        "    for model, accuracy in accuracies.items():\n",
        "        print(f\"  {model}: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12dabf55"
      },
      "source": [
        "## Display accuracies\n",
        "\n",
        "### Subtask:\n",
        "Present the extracted accuracy scores in a clear and readable format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4e46288"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `ml_results` variable was successfully processed to extract accuracy metrics for gradient boosting models (XGBoost and LightGBM) for classification tasks.\n",
        "*   For the 'is_diabetes_drug' target, both XGBoost and LightGBM models achieved an accuracy of 0.9405.\n",
        "*   For the 'high_efficacy' target, both XGBoost and LightGBM models achieved a perfect accuracy of 1.0000.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The models show consistent high performance across both gradient boosting algorithms for the given classification tasks, with particularly outstanding results for 'high_efficacy'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c586c3d0"
      },
      "source": [
        "# Task\n",
        "Retrieve and display the confusion matrices for all classification models found within the `ml_results` variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1949d9b8"
      },
      "source": [
        "## Retrieve and display confusion matrices\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the `ml_results` to find all classification models and print their respective confusion matrices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc89bcf5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   No data analysis was performed in this step as the solving process, which typically includes code execution and output, was not provided. The defined task is to retrieve and display confusion matrices specifically for classification models found within the `ml_results` variable.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The immediate next step is to implement and execute the necessary code to iterate through `ml_results`, identify classification models, and generate their respective confusion matrices. This execution will produce the data required for analysis and findings.\n"
      ]
    }
  ]
}